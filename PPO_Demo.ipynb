{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "nQnmcm0oI1Q-"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vekhJpsOxLK"
   },
   "source": [
    "# SBSim: A tutorial of using Reinforcement Learning for Optimizing Energy Use and Minimizing Carbon Emission in Office Buildings\n",
    "\n",
    "___\n",
    "\n",
    "Commercial office buildings contribute 17 percent of Carbon Emissions in the US, according to the US Energy Information Administration (EIA), and improving their efficiency will reduce their environmental burden and operating cost. A major contributor of energy consumption in these buildings are the Heating, Ventilation, and Air Conditioning (HVAC) devices. HVAC devices form a complex and interconnected thermodynamic system with the building and outside weather conditions, and current setpoint control policies are not fully optimized for minimizing energy use and carbon emission. Given a suitable training environment, a Reinforcement Learning (RL) agent is able to improve upon these policies, but training such a model, especially in a way that scales to thousands of buildings, presents many practical challenges. Most existing work on applying RL to this important task either makes use of proprietary data, or focuses on expensive and proprietary simulations that may not be grounded in the real world. We present the Smart Buildings Control Suite, the first open source interactive HVAC control dataset extracted from live sensor measurements of devices in real office buildings. The dataset consists of two components: real-world historical data from two buildings, for offline RL, and a lightweight interactive simulator for each of these buildings, calibrated using the historical data, for online and model-based RL. For ease of use, our RL environments are all compatible with the OpenAI gym environment standard. We believe this benchmark will accelerate progress and collaboration on HVAC optimization.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook accompanies the paper titled, **Real-World Data and Calibrated Simulation Suite for Offline Training of Reinforcement Learning Agents to Optimize Energy and Emission in Office Buildings** by Judah Goldfeder and John Sipple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7w-mjPcH7u6"
   },
   "source": [
    "# Smart Buildings Simulator Proximal Policy Optimization Demo\n",
    "\n",
    "This notebook runs through training a Proximal Policy Optimization agent on an HVAC building simulator that has been calibrated from real world data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "YchP7JXbSXS1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 21:43:51.880421: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-13 21:43:51.882904: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-13 21:43:51.969053: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-13 21:43:51.969110: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-13 21:43:51.970221: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-13 21:43:51.983075: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-13 21:43:51.988277: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-13 21:43:54.655965: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# @title Imports\n",
    "from dataclasses import dataclass\n",
    "import datetime, pytz\n",
    "import enum\n",
    "import functools\n",
    "import os\n",
    "import os\n",
    "import time\n",
    "from typing import Final, Sequence\n",
    "from typing import Optional\n",
    "from typing import Union, cast\n",
    "os.environ['WRAPT_DISABLE_EXTENSIONS'] = 'true'\n",
    "\n",
    "from absl import logging\n",
    "import gin\n",
    "import gin\n",
    "from matplotlib import patches\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import reverb\n",
    "import mediapy as media\n",
    "from IPython.display import clear_output\n",
    "from smart_control.environment import environment\n",
    "from smart_control.proto import smart_control_building_pb2\n",
    "from smart_control.proto import smart_control_normalization_pb2\n",
    "from smart_control.reward import electricity_energy_cost\n",
    "from smart_control.reward import natural_gas_energy_cost\n",
    "from smart_control.reward import setpoint_energy_carbon_regret\n",
    "from smart_control.reward import setpoint_energy_carbon_reward\n",
    "from smart_control.simulator import randomized_arrival_departure_occupancy\n",
    "from smart_control.simulator import rejection_simulator_building\n",
    "from smart_control.simulator import simulator_building\n",
    "from smart_control.simulator import step_function_occupancy\n",
    "from smart_control.simulator import stochastic_convection_simulator\n",
    "from smart_control.utils import bounded_action_normalizer\n",
    "from smart_control.utils import building_renderer\n",
    "from smart_control.utils import controller_reader\n",
    "from smart_control.utils import controller_writer\n",
    "from smart_control.utils import conversion_utils\n",
    "from smart_control.utils import observation_normalizer\n",
    "from smart_control.utils import reader_lib\n",
    "from smart_control.utils import writer_lib\n",
    "from smart_control.utils import histogram_reducer\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from smart_control.utils import environment_utils\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents.ppo import ppo_agent\n",
    "from tf_agents.agents.ppo import ppo_clip_agent\n",
    "from tf_agents.agents.ppo import ppo_actor_network\n",
    "from tf_agents.environments import parallel_py_environment\n",
    "from tf_agents.networks import value_network\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.keras_layers import inner_reshape\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.networks import nest_map\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import greedy_policy\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.policies import tf_policy\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.train import actor\n",
    "from tf_agents.train import learner\n",
    "from tf_agents.train import ppo_learner\n",
    "from tf_agents.train import triggers\n",
    "from tf_agents.train.utils import spec_utils\n",
    "from tf_agents.train.utils import train_utils\n",
    "from tf_agents.trajectories import policy_step\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.trajectories import trajectory as trajectory_lib\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.typing import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "sDDU5FmLkYo-"
   },
   "outputs": [],
   "source": [
    "# @title Set local runtime configurations\n",
    "def logging_info(*args):\n",
    "  logging.info(*args)\n",
    "  print(*args)\n",
    "\n",
    "data_path = \"/home/derek/sbsim/smart_control/configs/resources/sb1/\" #@param {type:\"string\"}\n",
    "metrics_path = \"/home/derek/sbsim/metrics/\" #@param {type:\"string\"}\n",
    "output_data_path = '/home/derek/sbsim/smart_control/sb_colab_demo/' #@param {type:\"string\"}\n",
    "root_dir = \"/home/derek/sbsim/\" #@param {type:\"string\"}\n",
    "\n",
    "\n",
    "# \n",
    "@gin.configurable\n",
    "def get_histogram_reducer():\n",
    "\n",
    "\n",
    "    reader = controller_reader.ProtoReader(data_path)\n",
    "\n",
    "    hr = histogram_reducer.HistogramReducer(\n",
    "        histogram_parameters_tuples=histogram_parameters_tuples,\n",
    "        reader=reader,\n",
    "        normalize_reduce=True,\n",
    "        )\n",
    "    return hr\n",
    "\n",
    "!mkdir -p $root_dir\n",
    "!mkdir -p $output_data_path\n",
    "!mkdir -p $metrics_path\n",
    "\n",
    "def remap_filepath(filepath) -> str:\n",
    "    return filepath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "JV_2oCn2uQU4"
   },
   "outputs": [],
   "source": [
    "# @title Plotting Utities\n",
    "reward_shift = 0\n",
    "reward_scale = 1.0\n",
    "person_productivity_hour = 300.0\n",
    "\n",
    "KELVIN_TO_CELSIUS = 273.15\n",
    "\n",
    "\n",
    "def render_env(env: environment.Environment):\n",
    "  \"\"\"Renders the environment.\"\"\"\n",
    "  building_layout = env.building._simulator._building._floor_plan\n",
    "\n",
    "  # create a renderer\n",
    "  renderer = building_renderer.BuildingRenderer(building_layout, 1)\n",
    "\n",
    "  # get the current temps to render\n",
    "  # this also is not ideal, since the temps are not fully exposed.\n",
    "  # V Ideally this should be a publicly accessable field\n",
    "  temps = env.building._simulator._building.temp\n",
    "\n",
    "  input_q = env.building._simulator._building.input_q\n",
    "\n",
    "  # render\n",
    "  vmin = 285\n",
    "  vmax = 305\n",
    "  image = renderer.render(\n",
    "      temps,\n",
    "      cmap='bwr',\n",
    "      vmin=vmin,\n",
    "      vmax=vmax,\n",
    "      colorbar=False,\n",
    "      input_q=input_q,\n",
    "      diff_range=0.5,\n",
    "      diff_size=1,\n",
    "  ).convert('RGB')\n",
    "  media.show_image(\n",
    "      image, title='Environment %s' % env.current_simulation_timestamp\n",
    "  )\n",
    "\n",
    "\n",
    "def get_energy_timeseries(reward_infos, time_zone: str) -> pd.DataFrame:\n",
    "  \"\"\"Returns a timeseries of energy rates.\"\"\"\n",
    "\n",
    "  start_times = []\n",
    "  end_times = []\n",
    "\n",
    "  device_ids = []\n",
    "  device_types = []\n",
    "  air_handler_blower_electrical_energy_rates = []\n",
    "  air_handler_air_conditioner_energy_rates = []\n",
    "  boiler_natural_gas_heating_energy_rates = []\n",
    "  boiler_pump_electrical_energy_rates = []\n",
    "\n",
    "  for reward_info in reward_infos:\n",
    "    end_timestamp = conversion_utils.proto_to_pandas_timestamp(\n",
    "        reward_info.end_timestamp\n",
    "    ).tz_convert(time_zone)\n",
    "    start_timestamp = end_timestamp - pd.Timedelta(300, unit='second')\n",
    "\n",
    "    for air_handler_id in reward_info.air_handler_reward_infos:\n",
    "      start_times.append(start_timestamp)\n",
    "      end_times.append(end_timestamp)\n",
    "\n",
    "      device_ids.append(air_handler_id)\n",
    "      device_types.append('air_handler')\n",
    "\n",
    "      air_handler_blower_electrical_energy_rates.append(\n",
    "          reward_info.air_handler_reward_infos[\n",
    "              air_handler_id\n",
    "          ].blower_electrical_energy_rate\n",
    "      )\n",
    "      air_handler_air_conditioner_energy_rates.append(\n",
    "          reward_info.air_handler_reward_infos[\n",
    "              air_handler_id\n",
    "          ].air_conditioning_electrical_energy_rate\n",
    "      )\n",
    "      boiler_natural_gas_heating_energy_rates.append(0)\n",
    "      boiler_pump_electrical_energy_rates.append(0)\n",
    "\n",
    "    for boiler_id in reward_info.boiler_reward_infos:\n",
    "      start_times.append(start_timestamp)\n",
    "      end_times.append(end_timestamp)\n",
    "\n",
    "      device_ids.append(boiler_id)\n",
    "      device_types.append('boiler')\n",
    "\n",
    "      air_handler_blower_electrical_energy_rates.append(0)\n",
    "      air_handler_air_conditioner_energy_rates.append(0)\n",
    "\n",
    "      boiler_natural_gas_heating_energy_rates.append(\n",
    "          reward_info.boiler_reward_infos[\n",
    "              boiler_id\n",
    "          ].natural_gas_heating_energy_rate\n",
    "      )\n",
    "      boiler_pump_electrical_energy_rates.append(\n",
    "          reward_info.boiler_reward_infos[boiler_id].pump_electrical_energy_rate\n",
    "      )\n",
    "\n",
    "  df_map = {\n",
    "      'start_time': start_times,\n",
    "      'end_time': end_times,\n",
    "      'device_id': device_ids,\n",
    "      'device_type': device_types,\n",
    "      'air_handler_blower_electrical_energy_rate': (\n",
    "          air_handler_blower_electrical_energy_rates\n",
    "      ),\n",
    "      'air_handler_air_conditioner_energy_rate': (\n",
    "          air_handler_air_conditioner_energy_rates\n",
    "      ),\n",
    "      'boiler_natural_gas_heating_energy_rate': (\n",
    "          boiler_natural_gas_heating_energy_rates\n",
    "      ),\n",
    "      'boiler_pump_electrical_energy_rate': boiler_pump_electrical_energy_rates,\n",
    "  }\n",
    "  df = pd.DataFrame(df_map).sort_values('start_time')\n",
    "  return df\n",
    "\n",
    "\n",
    "def get_outside_air_temperature_timeseries(\n",
    "    observation_responses,\n",
    "    time_zone: str,\n",
    ") -> pd.Series:\n",
    "  \"\"\"Returns a timeseries of outside air temperature.\"\"\"\n",
    "  temps = []\n",
    "  for i in range(len(observation_responses)):\n",
    "    temp = [\n",
    "        (\n",
    "            conversion_utils.proto_to_pandas_timestamp(\n",
    "                sor.timestamp\n",
    "            ).tz_convert(time_zone)\n",
    "            - pd.Timedelta(300, unit='second'),\n",
    "            sor.continuous_value,\n",
    "        )\n",
    "        for sor in observation_responses[i].single_observation_responses\n",
    "        if sor.single_observation_request.measurement_name\n",
    "        == 'outside_air_temperature_sensor'\n",
    "    ][0]\n",
    "    temps.append(temp)\n",
    "\n",
    "  res = list(zip(*temps))\n",
    "  return pd.Series(res[1], index=res[0]).sort_index()\n",
    "\n",
    "\n",
    "def get_reward_timeseries(\n",
    "    reward_infos,\n",
    "    reward_responses,\n",
    "    time_zone: str,\n",
    ") -> pd.DataFrame:\n",
    "  \"\"\"Returns a timeseries of reward values.\"\"\"\n",
    "  cols = [\n",
    "      'agent_reward_value',\n",
    "      'electricity_energy_cost',\n",
    "      'carbon_emitted',\n",
    "      'occupancy',\n",
    "  ]\n",
    "  df = pd.DataFrame(columns=cols)\n",
    "\n",
    "  for i in range(min(len(reward_responses), len(reward_infos))):\n",
    "    step_start_timestamp = conversion_utils.proto_to_pandas_timestamp(\n",
    "        reward_infos[i].start_timestamp\n",
    "    ).tz_convert(time_zone)\n",
    "    step_end_timestamp = conversion_utils.proto_to_pandas_timestamp(\n",
    "        reward_infos[i].end_timestamp\n",
    "    ).tz_convert(time_zone)\n",
    "    delta_time_sec = (step_end_timestamp - step_start_timestamp).total_seconds()\n",
    "    occupancy = np.sum([\n",
    "        reward_infos[i].zone_reward_infos[zone_id].average_occupancy\n",
    "        for zone_id in reward_infos[i].zone_reward_infos\n",
    "    ])\n",
    "\n",
    "    df.loc[\n",
    "        conversion_utils.proto_to_pandas_timestamp(\n",
    "            reward_infos[i].start_timestamp\n",
    "        ).tz_convert(time_zone)\n",
    "    ] = [\n",
    "        reward_responses[i].agent_reward_value,\n",
    "        reward_responses[i].electricity_energy_cost,\n",
    "        reward_responses[i].carbon_emitted,\n",
    "        occupancy,\n",
    "    ]\n",
    "\n",
    "  df = df.sort_index()\n",
    "  df['cumulative_reward'] = df['agent_reward_value'].cumsum()\n",
    "  logging_info('Cumulative reward: %4.2f' % df.iloc[-1]['cumulative_reward'])\n",
    "  return df\n",
    "\n",
    "\n",
    "def format_plot(\n",
    "    ax1, xlabel: str, start_time: int, end_time: int, time_zone: str\n",
    "):\n",
    "  \"\"\"Formats a plot with common attributes.\"\"\"\n",
    "  ax1.set_facecolor('black')\n",
    "  ax1.xaxis.tick_top()\n",
    "  ax1.tick_params(axis='x', labelsize=12)\n",
    "  ax1.tick_params(axis='y', labelsize=12)\n",
    "  ax1.xaxis.set_major_formatter(\n",
    "      mdates.DateFormatter('%a %m/%d %H:%M', tz=pytz.timezone(time_zone))\n",
    "  )\n",
    "  ax1.grid(color='gray', linestyle='-', linewidth=1.0)\n",
    "  ax1.set_ylabel(xlabel, color='blue', fontsize=12)\n",
    "  ax1.set_xlim(left=start_time, right=end_time)\n",
    "  ax1.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "  ax1.legend(prop={'size': 10})\n",
    "\n",
    "\n",
    "def plot_occupancy_timeline(\n",
    "    ax1, reward_timeseries: pd.DataFrame, time_zone: str\n",
    "):\n",
    "  local_times = [ts.tz_convert(time_zone) for ts in reward_timeseries.index]\n",
    "  ax1.plot(\n",
    "      local_times,\n",
    "      reward_timeseries['occupancy'],\n",
    "      color='cyan',\n",
    "      marker=None,\n",
    "      alpha=1,\n",
    "      lw=2,\n",
    "      linestyle='-',\n",
    "      label='Num Occupants',\n",
    "  )\n",
    "  format_plot(\n",
    "      ax1,\n",
    "      'Occupancy',\n",
    "      reward_timeseries.index.min(),\n",
    "      reward_timeseries.index.max(),\n",
    "      time_zone,\n",
    "  )\n",
    "\n",
    "\n",
    "def plot_energy_cost_timeline(\n",
    "    ax1,\n",
    "    reward_timeseries: pd.DataFrame,\n",
    "    time_zone: str,\n",
    "    cumulative: bool = False,\n",
    "):\n",
    "  local_times = [ts.tz_convert(time_zone) for ts in reward_timeseries.index]\n",
    "  if cumulative:\n",
    "    feature_timeseries_cost = reward_timeseries[\n",
    "        'electricity_energy_cost'\n",
    "    ].cumsum()\n",
    "  else:\n",
    "    feature_timeseries_cost = reward_timeseries['electricity_energy_cost']\n",
    "  ax1.plot(\n",
    "      local_times,\n",
    "      feature_timeseries_cost,\n",
    "      color='magenta',\n",
    "      marker=None,\n",
    "      alpha=1,\n",
    "      lw=2,\n",
    "      linestyle='-',\n",
    "      label='Electricity',\n",
    "  )\n",
    "\n",
    "  format_plot(\n",
    "      ax1,\n",
    "      'Energy Cost [$]',\n",
    "      reward_timeseries.index.min(),\n",
    "      reward_timeseries.index.max(),\n",
    "      time_zone,\n",
    "  )\n",
    "\n",
    "\n",
    "def plot_reward_timeline(ax1, reward_timeseries, time_zone):\n",
    "\n",
    "  local_times = [ts.tz_convert(time_zone) for ts in reward_timeseries.index]\n",
    "\n",
    "  ax1.plot(\n",
    "      local_times,\n",
    "      reward_timeseries['cumulative_reward'],\n",
    "      color='royalblue',\n",
    "      marker=None,\n",
    "      alpha=1,\n",
    "      lw=6,\n",
    "      linestyle='-',\n",
    "      label='reward',\n",
    "  )\n",
    "  format_plot(\n",
    "      ax1,\n",
    "      'Agent Reward',\n",
    "      reward_timeseries.index.min(),\n",
    "      reward_timeseries.index.max(),\n",
    "      time_zone,\n",
    "  )\n",
    "\n",
    "\n",
    "def plot_energy_timeline(ax1, energy_timeseries, time_zone, cumulative=False):\n",
    "\n",
    "  def _to_kwh(\n",
    "      energy_rate: float,\n",
    "      step_interval: pd.Timedelta = pd.Timedelta(5, unit='minute'),\n",
    "  ) -> float:\n",
    "    kw_power = energy_rate / 1000.0\n",
    "    hwh_power = kw_power * step_interval / pd.Timedelta(1, unit='hour')\n",
    "    return hwh_power.cumsum()\n",
    "\n",
    "  timeseries = energy_timeseries[\n",
    "      energy_timeseries['device_type'] == 'air_handler'\n",
    "  ]\n",
    "\n",
    "  if cumulative:\n",
    "    feature_timeseries_ac = _to_kwh(\n",
    "        timeseries['air_handler_air_conditioner_energy_rate']\n",
    "    )\n",
    "    feature_timeseries_blower = _to_kwh(\n",
    "        timeseries['air_handler_blower_electrical_energy_rate']\n",
    "    )\n",
    "  else:\n",
    "    feature_timeseries_ac = (\n",
    "        timeseries['air_handler_air_conditioner_energy_rate'] / 1000.0\n",
    "    )\n",
    "    feature_timeseries_blower = (\n",
    "        timeseries['air_handler_blower_electrical_energy_rate'] / 1000.0\n",
    "    )\n",
    "\n",
    "  ax1.plot(\n",
    "      timeseries['start_time'],\n",
    "      feature_timeseries_ac,\n",
    "      color='magenta',\n",
    "      marker=None,\n",
    "      alpha=1,\n",
    "      lw=4,\n",
    "      linestyle='-',\n",
    "      label='AHU Electricity',\n",
    "  )\n",
    "  ax1.plot(\n",
    "      timeseries['start_time'],\n",
    "      feature_timeseries_blower,\n",
    "      color='magenta',\n",
    "      marker=None,\n",
    "      alpha=1,\n",
    "      lw=4,\n",
    "      linestyle='--',\n",
    "      label='FAN Electricity',\n",
    "  )\n",
    "\n",
    "  timeseries = energy_timeseries[energy_timeseries['device_type'] == 'boiler']\n",
    "  if cumulative:\n",
    "    feature_timeseries_gas = _to_kwh(\n",
    "        timeseries['boiler_natural_gas_heating_energy_rate']\n",
    "    )\n",
    "    feature_timeseries_pump = _to_kwh(\n",
    "        timeseries['boiler_pump_electrical_energy_rate']\n",
    "    )\n",
    "  else:\n",
    "    feature_timeseries_gas = (\n",
    "        timeseries['boiler_natural_gas_heating_energy_rate'] / 1000.0\n",
    "    )\n",
    "    feature_timeseries_pump = (\n",
    "        timeseries['boiler_pump_electrical_energy_rate'] / 1000.0\n",
    "    )\n",
    "\n",
    "  ax1.plot(\n",
    "      timeseries['start_time'],\n",
    "      feature_timeseries_gas,\n",
    "      color='lime',\n",
    "      marker=None,\n",
    "      alpha=1,\n",
    "      lw=4,\n",
    "      linestyle='-',\n",
    "      label='BLR Gas',\n",
    "  )\n",
    "  ax1.plot(\n",
    "      timeseries['start_time'],\n",
    "      feature_timeseries_pump,\n",
    "      color='lime',\n",
    "      marker=None,\n",
    "      alpha=1,\n",
    "      lw=4,\n",
    "      linestyle='--',\n",
    "      label='Pump Electricity',\n",
    "  )\n",
    "\n",
    "  if cumulative:\n",
    "    label = 'HVAC Energy Consumption [kWh]'\n",
    "  else:\n",
    "    label = 'HVAC Power Consumption [kW]'\n",
    "\n",
    "  format_plot(\n",
    "      ax1,\n",
    "      label,\n",
    "      timeseries['start_time'].min(),\n",
    "      timeseries['end_time'].max(),\n",
    "      time_zone,\n",
    "  )\n",
    "\n",
    "\n",
    "def plot_carbon_timeline(ax1, reward_timeseries, time_zone, cumulative=False):\n",
    "  \"\"\"Plots carbon-emission timeline.\"\"\"\n",
    "\n",
    "  if cumulative:\n",
    "    feature_timeseries_carbon = reward_timeseries['carbon_emitted'].cumsum()\n",
    "  else:\n",
    "    feature_timeseries_carbon = reward_timeseries['carbon_emitted']\n",
    "  ax1.plot(\n",
    "      reward_timeseries.index,\n",
    "      feature_timeseries_carbon,\n",
    "      color='white',\n",
    "      marker=None,\n",
    "      alpha=1,\n",
    "      lw=4,\n",
    "      linestyle='-',\n",
    "      label='Carbon',\n",
    "  )\n",
    "  format_plot(\n",
    "      ax1,\n",
    "      'Carbon emission [kg]',\n",
    "      reward_timeseries.index.min(),\n",
    "      reward_timeseries.index.max(),\n",
    "      time_zone,\n",
    "  )\n",
    "\n",
    "\n",
    "def get_zone_timeseries(reward_infos, time_zone):\n",
    "  \"\"\"Converts reward infos to a timeseries dataframe.\"\"\"\n",
    "\n",
    "  start_times = []\n",
    "  end_times = []\n",
    "  zones = []\n",
    "  heating_setpoints = []\n",
    "  cooling_setpoints = []\n",
    "  zone_air_temperatures = []\n",
    "  air_flow_rate_setpoints = []\n",
    "  air_flow_rates = []\n",
    "  average_occupancies = []\n",
    "\n",
    "  for reward_info in reward_infos:\n",
    "    start_timestamp = conversion_utils.proto_to_pandas_timestamp(\n",
    "        reward_info.end_timestamp\n",
    "    ).tz_convert(time_zone) - pd.Timedelta(300, unit='second')\n",
    "    end_timestamp = conversion_utils.proto_to_pandas_timestamp(\n",
    "        reward_info.end_timestamp\n",
    "    ).tz_convert(time_zone)\n",
    "\n",
    "    for zone_id in reward_info.zone_reward_infos:\n",
    "      zones.append(zone_id)\n",
    "      start_times.append(start_timestamp)\n",
    "      end_times.append(end_timestamp)\n",
    "\n",
    "      heating_setpoints.append(\n",
    "          reward_info.zone_reward_infos[zone_id].heating_setpoint_temperature\n",
    "      )\n",
    "      cooling_setpoints.append(\n",
    "          reward_info.zone_reward_infos[zone_id].cooling_setpoint_temperature\n",
    "      )\n",
    "\n",
    "      zone_air_temperatures.append(\n",
    "          reward_info.zone_reward_infos[zone_id].zone_air_temperature\n",
    "      )\n",
    "      air_flow_rate_setpoints.append(\n",
    "          reward_info.zone_reward_infos[zone_id].air_flow_rate_setpoint\n",
    "      )\n",
    "      air_flow_rates.append(\n",
    "          reward_info.zone_reward_infos[zone_id].air_flow_rate\n",
    "      )\n",
    "      average_occupancies.append(\n",
    "          reward_info.zone_reward_infos[zone_id].average_occupancy\n",
    "      )\n",
    "\n",
    "  df_map = {\n",
    "      'start_time': start_times,\n",
    "      'end_time': end_times,\n",
    "      'zone': zones,\n",
    "      'heating_setpoint_temperature': heating_setpoints,\n",
    "      'cooling_setpoint_temperature': cooling_setpoints,\n",
    "      'zone_air_temperature': zone_air_temperatures,\n",
    "      'air_flow_rate_setpoint': air_flow_rate_setpoints,\n",
    "      'air_flow_rate': air_flow_rates,\n",
    "      'average_occupancy': average_occupancies,\n",
    "  }\n",
    "  return pd.DataFrame(df_map).sort_values('start_time')\n",
    "\n",
    "\n",
    "def get_action_timeseries(action_responses):\n",
    "  \"\"\"Converts action responses to a dataframe.\"\"\"\n",
    "  timestamps = []\n",
    "  device_ids = []\n",
    "  setpoint_names = []\n",
    "  setpoint_values = []\n",
    "  response_types = []\n",
    "  for action_response in action_responses:\n",
    "\n",
    "    timestamp = conversion_utils.proto_to_pandas_timestamp(\n",
    "        action_response.timestamp\n",
    "    )\n",
    "    for single_action_response in action_response.single_action_responses:\n",
    "      device_id = single_action_response.request.device_id\n",
    "      setpoint_name = single_action_response.request.setpoint_name\n",
    "      setpoint_value = single_action_response.request.continuous_value\n",
    "      response_type = single_action_response.response_type\n",
    "\n",
    "      timestamps.append(timestamp)\n",
    "      device_ids.append(device_id)\n",
    "      setpoint_names.append(setpoint_name)\n",
    "      setpoint_values.append(setpoint_value)\n",
    "      response_types.append(response_type)\n",
    "\n",
    "  return pd.DataFrame({\n",
    "      'timestamp': timestamps,\n",
    "      'device_id': device_ids,\n",
    "      'setpoint_name': setpoint_names,\n",
    "      'setpoint_value': setpoint_values,\n",
    "      'response_type': response_types,\n",
    "  })\n",
    "\n",
    "\n",
    "def plot_action_timeline(ax1, action_timeseries, action_tuple, time_zone):\n",
    "  \"\"\"Plots action timeline.\"\"\"\n",
    "\n",
    "  single_action_timeseries = action_timeseries[\n",
    "      (action_timeseries['device_id'] == action_tuple[0])\n",
    "      & (action_timeseries['setpoint_name'] == action_tuple[1])\n",
    "  ]\n",
    "  single_action_timeseries = single_action_timeseries.sort_values(\n",
    "      by='timestamp'\n",
    "  )\n",
    "\n",
    "  if action_tuple[1] in [\n",
    "      'supply_water_setpoint',\n",
    "      'supply_air_heating_temperature_setpoint',\n",
    "  ]:\n",
    "    single_action_timeseries['setpoint_value'] = (\n",
    "        single_action_timeseries['setpoint_value'] - KELVIN_TO_CELSIUS\n",
    "    )\n",
    "\n",
    "  ax1.plot(\n",
    "      single_action_timeseries['timestamp'],\n",
    "      single_action_timeseries['setpoint_value'],\n",
    "      color='lime',\n",
    "      marker=None,\n",
    "      alpha=1,\n",
    "      lw=4,\n",
    "      linestyle='-',\n",
    "      label=action_tuple[1],\n",
    "  )\n",
    "  title = '%s %s' % (action_tuple[0], action_tuple[1])\n",
    "  format_plot(\n",
    "      ax1,\n",
    "      'Action',\n",
    "      single_action_timeseries['timestamp'].min(),\n",
    "      single_action_timeseries['timestamp'].max(),\n",
    "      time_zone,\n",
    "  )\n",
    "\n",
    "\n",
    "def get_outside_air_temperature_timeseries(observation_responses, time_zone):\n",
    "  temps = []\n",
    "  for i in range(len(observation_responses)):\n",
    "    temp = [\n",
    "        (\n",
    "            conversion_utils.proto_to_pandas_timestamp(\n",
    "                sor.timestamp\n",
    "            ).tz_convert(time_zone),\n",
    "            sor.continuous_value,\n",
    "        )\n",
    "        for sor in observation_responses[i].single_observation_responses\n",
    "        if sor.single_observation_request.measurement_name\n",
    "        == 'outside_air_temperature_sensor'\n",
    "    ][0]\n",
    "    temps.append(temp)\n",
    "\n",
    "  res = list(zip(*temps))\n",
    "  return pd.Series(res[1], index=res[0]).sort_index()\n",
    "\n",
    "\n",
    "def plot_temperature_timeline(\n",
    "    ax1, zone_timeseries, outside_air_temperature_timeseries, time_zone\n",
    "):\n",
    "  zone_temps = pd.pivot_table(\n",
    "      zone_timeseries,\n",
    "      index=zone_timeseries['start_time'],\n",
    "      columns='zone',\n",
    "      values='zone_air_temperature',\n",
    "  ).sort_index()\n",
    "  zone_temps.quantile(q=0.25, axis=1)\n",
    "  zone_temp_stats = pd.DataFrame({\n",
    "      'min_temp': zone_temps.min(axis=1),\n",
    "      'q25_temp': zone_temps.quantile(q=0.25, axis=1),\n",
    "      'median_temp': zone_temps.median(axis=1),\n",
    "      'q75_temp': zone_temps.quantile(q=0.75, axis=1),\n",
    "      'max_temp': zone_temps.max(axis=1),\n",
    "  })\n",
    "\n",
    "  zone_heating_setpoints = (\n",
    "      pd.pivot_table(\n",
    "          zone_timeseries,\n",
    "          index=zone_timeseries['start_time'],\n",
    "          columns='zone',\n",
    "          values='heating_setpoint_temperature',\n",
    "      )\n",
    "      .sort_index()\n",
    "      .min(axis=1)\n",
    "  )\n",
    "  zone_cooling_setpoints = (\n",
    "      pd.pivot_table(\n",
    "          zone_timeseries,\n",
    "          index=zone_timeseries['start_time'],\n",
    "          columns='zone',\n",
    "          values='cooling_setpoint_temperature',\n",
    "      )\n",
    "      .sort_index()\n",
    "      .max(axis=1)\n",
    "  )\n",
    "\n",
    "  ax1.plot(\n",
    "      zone_cooling_setpoints.index,\n",
    "      zone_cooling_setpoints - KELVIN_TO_CELSIUS,\n",
    "      color='yellow',\n",
    "      lw=1,\n",
    "  )\n",
    "  ax1.plot(\n",
    "      zone_cooling_setpoints.index,\n",
    "      zone_heating_setpoints - KELVIN_TO_CELSIUS,\n",
    "      color='yellow',\n",
    "      lw=1,\n",
    "  )\n",
    "\n",
    "  ax1.fill_between(\n",
    "      zone_temp_stats.index,\n",
    "      zone_temp_stats['min_temp'] - KELVIN_TO_CELSIUS,\n",
    "      zone_temp_stats['max_temp'] - KELVIN_TO_CELSIUS,\n",
    "      facecolor='green',\n",
    "      alpha=0.8,\n",
    "  )\n",
    "  ax1.fill_between(\n",
    "      zone_temp_stats.index,\n",
    "      zone_temp_stats['q25_temp'] - KELVIN_TO_CELSIUS,\n",
    "      zone_temp_stats['q75_temp'] - KELVIN_TO_CELSIUS,\n",
    "      facecolor='green',\n",
    "      alpha=0.8,\n",
    "  )\n",
    "  ax1.plot(\n",
    "      zone_temp_stats.index,\n",
    "      zone_temp_stats['median_temp'] - KELVIN_TO_CELSIUS,\n",
    "      color='white',\n",
    "      lw=3,\n",
    "      alpha=1.0,\n",
    "  )\n",
    "  ax1.plot(\n",
    "      outside_air_temperature_timeseries.index,\n",
    "      outside_air_temperature_timeseries - KELVIN_TO_CELSIUS,\n",
    "      color='magenta',\n",
    "      lw=3,\n",
    "      alpha=1.0,\n",
    "  )\n",
    "  format_plot(\n",
    "      ax1,\n",
    "      'Temperature [C]',\n",
    "      zone_temp_stats.index.min(),\n",
    "      zone_temp_stats.index.max(),\n",
    "      time_zone,\n",
    "  )\n",
    "\n",
    "\n",
    "def plot_timeseries_charts(reader, time_zone):\n",
    "  \"\"\"Plots timeseries charts.\"\"\"\n",
    "\n",
    "  observation_responses = reader.read_observation_responses(\n",
    "      pd.Timestamp.min, pd.Timestamp.max\n",
    "  )\n",
    "  action_responses = reader.read_action_responses(\n",
    "      pd.Timestamp.min, pd.Timestamp.max\n",
    "  )\n",
    "  reward_infos = reader.read_reward_infos(pd.Timestamp.min, pd.Timestamp.max)\n",
    "  reward_responses = reader.read_reward_responses(\n",
    "      pd.Timestamp.min, pd.Timestamp.max\n",
    "  )\n",
    "\n",
    "  if len(reward_infos) == 0 or len(reward_responses) == 0:\n",
    "    return\n",
    "\n",
    "  action_timeseries = get_action_timeseries(action_responses)\n",
    "  action_tuples = list(\n",
    "      set([\n",
    "          (row['device_id'], row['setpoint_name'])\n",
    "          for _, row in action_timeseries.iterrows()\n",
    "      ])\n",
    "  )\n",
    "\n",
    "  reward_timeseries = get_reward_timeseries(\n",
    "      reward_infos, reward_responses, time_zone\n",
    "  ).sort_index()\n",
    "  outside_air_temperature_timeseries = get_outside_air_temperature_timeseries(\n",
    "      observation_responses, time_zone\n",
    "  )\n",
    "  zone_timeseries = get_zone_timeseries(reward_infos, time_zone)\n",
    "  fig, axes = plt.subplots(\n",
    "      nrows=6 + len(action_tuples),\n",
    "      ncols=1,\n",
    "      gridspec_kw={\n",
    "          'height_ratios': [1, 1, 1, 1, 1, 1] + [1] * len(action_tuples)\n",
    "      },\n",
    "      squeeze=True,\n",
    "  )\n",
    "  fig.set_size_inches(24, 25)\n",
    "\n",
    "  energy_timeseries = get_energy_timeseries(reward_infos, time_zone)\n",
    "  plot_reward_timeline(axes[0], reward_timeseries, time_zone)\n",
    "  plot_energy_timeline(axes[1], energy_timeseries, time_zone, cumulative=True)\n",
    "  plot_energy_cost_timeline(\n",
    "      axes[2], reward_timeseries, time_zone, cumulative=True\n",
    "  )\n",
    "  plot_carbon_timeline(axes[3], reward_timeseries, time_zone, cumulative=True)\n",
    "  plot_occupancy_timeline(axes[4], reward_timeseries, time_zone)\n",
    "  plot_temperature_timeline(\n",
    "      axes[5], zone_timeseries, outside_air_temperature_timeseries, time_zone\n",
    "  )\n",
    "\n",
    "  for i, action_tuple in enumerate(action_tuples):\n",
    "    plot_action_timeline(\n",
    "        axes[6 + i], action_timeseries, action_tuple, time_zone\n",
    "    )\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTtVb9wbRsKU"
   },
   "source": [
    "# Load up the environment\n",
    "\n",
    "In this section we load up the Smart Buildings simulator environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "id": "2fcYS1VBrvia"
   },
   "outputs": [],
   "source": [
    "# @title Utils for importing the environment.\n",
    "\n",
    "def load_environment(gin_config_file: str):\n",
    "  \"\"\"Returns an Environment from a config file.\"\"\"\n",
    "  # Global definition is required by Gin library to instantiate Environment.\n",
    "  global environment  # pylint: disable=global-variable-not-assigned\n",
    "  with gin.unlock_config():\n",
    "    gin.parse_config_file(gin_config_file)\n",
    "    return environment.Environment()  # pylint: disable=no-value-for-parameter\n",
    "\n",
    "\n",
    "def get_latest_episode_reader(\n",
    "    metrics_path: str,\n",
    ") -> controller_reader.ProtoReader:\n",
    "\n",
    "  episode_infos = controller_reader.get_episode_data(metrics_path).sort_index()\n",
    "  selected_episode = episode_infos.index[-1]\n",
    "  episode_path = os.path.join(metrics_path, selected_episode)\n",
    "  reader = controller_reader.ProtoReader(episode_path)\n",
    "  return reader\n",
    "\n",
    "@gin.configurable\n",
    "def get_histogram_path():\n",
    "  return data_path\n",
    "\n",
    "\n",
    "@gin.configurable\n",
    "def get_reset_temp_values():\n",
    "  reset_temps_filepath = remap_filepath(\n",
    "      os.path.join(data_path, \"reset_temps.npy\")\n",
    "  )\n",
    "\n",
    "  return np.load(reset_temps_filepath)\n",
    "\n",
    "\n",
    "@gin.configurable\n",
    "def get_zone_path():\n",
    "  return remap_filepath(\n",
    "      os.path.join(data_path, \"double_resolution_zone_1_2.npy\")\n",
    "  )\n",
    "\n",
    "\n",
    "@gin.configurable\n",
    "def get_metrics_path():\n",
    "  return os.path.join(metrics_path, \"metrics\")\n",
    "\n",
    "\n",
    "@gin.configurable\n",
    "def get_weather_path():\n",
    "  return remap_filepath(\n",
    "      os.path.join(\n",
    "          data_path, \"local_weather_moffett_field_20230701_20231122.csv\"\n",
    "      )\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10THzl_rSgFW"
   },
   "source": [
    "In the cell below, we will load the collect and eval environments. While we are loading the same environment, below, it would be useful to load the same building over near, but non-overlapping time windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "id": "XFeGO2TLRS1o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/derek/sbsim/smart_control/configs/resources/sb1/sim_config.gin\n",
      "/home/derek/sbsim/smart_control/configs/resources/sb1/sim_config.gin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/derek/sbsim/smart_control/simulator/building_utils.py:283: UserWarning: Connected components is showing that there are 4 or fewer\n",
      "     rooms in your building. You may have your 0's and 1's inverted in the\n",
      "     floor_plan. Remember that for the connectedComponents function,\n",
      "     0's must code for exterior space and exterior or interior walls,\n",
      "     and 1's must code for interior space.\n",
      "  warnings.warn(\"\"\"Connected components is showing that there are 4 or fewer\n",
      "2024-11-13 21:44:34.750814: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-13 21:44:34.751136: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-13 21:44:34.751623: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "/home/derek/sbsim/smart_control/reward/electricity_energy_cost.py:147: UnitStrippedWarning: The unit of the quantity is stripped when downcasting to ndarray.\n",
      "  np.array(carbon_emission_rates) / 1.0e6 / 3600.0\n",
      "/home/derek/sbsim/smart_control/reward/electricity_energy_cost.py:152: UnitStrippedWarning: The unit of the quantity is stripped when downcasting to ndarray.\n",
      "  np.array(weekday_energy_prices)\n",
      "/home/derek/sbsim/smart_control/reward/electricity_energy_cost.py:159: UnitStrippedWarning: The unit of the quantity is stripped when downcasting to ndarray.\n",
      "  np.array(weekend_energy_prices)\n",
      "/home/derek/sbsim/smart_control/simulator/building_utils.py:283: UserWarning: Connected components is showing that there are 4 or fewer\n",
      "     rooms in your building. You may have your 0's and 1's inverted in the\n",
      "     floor_plan. Remember that for the connectedComponents function,\n",
      "     0's must code for exterior space and exterior or interior walls,\n",
      "     and 1's must code for interior space.\n",
      "  warnings.warn(\"\"\"Connected components is showing that there are 4 or fewer\n",
      "/home/derek/sbsim/smart_control/reward/electricity_energy_cost.py:147: UnitStrippedWarning: The unit of the quantity is stripped when downcasting to ndarray.\n",
      "  np.array(carbon_emission_rates) / 1.0e6 / 3600.0\n",
      "/home/derek/sbsim/smart_control/reward/electricity_energy_cost.py:152: UnitStrippedWarning: The unit of the quantity is stripped when downcasting to ndarray.\n",
      "  np.array(weekday_energy_prices)\n",
      "/home/derek/sbsim/smart_control/reward/electricity_energy_cost.py:159: UnitStrippedWarning: The unit of the quantity is stripped when downcasting to ndarray.\n",
      "  np.array(weekend_energy_prices)\n",
      "/home/derek/sbsim/smart_control/simulator/building_utils.py:283: UserWarning: Connected components is showing that there are 4 or fewer\n",
      "     rooms in your building. You may have your 0's and 1's inverted in the\n",
      "     floor_plan. Remember that for the connectedComponents function,\n",
      "     0's must code for exterior space and exterior or interior walls,\n",
      "     and 1's must code for interior space.\n",
      "  warnings.warn(\"\"\"Connected components is showing that there are 4 or fewer\n",
      "/home/derek/sbsim/smart_control/reward/electricity_energy_cost.py:147: UnitStrippedWarning: The unit of the quantity is stripped when downcasting to ndarray.\n",
      "  np.array(carbon_emission_rates) / 1.0e6 / 3600.0\n",
      "/home/derek/sbsim/smart_control/reward/electricity_energy_cost.py:152: UnitStrippedWarning: The unit of the quantity is stripped when downcasting to ndarray.\n",
      "  np.array(weekday_energy_prices)\n",
      "/home/derek/sbsim/smart_control/reward/electricity_energy_cost.py:159: UnitStrippedWarning: The unit of the quantity is stripped when downcasting to ndarray.\n",
      "  np.array(weekend_energy_prices)\n"
     ]
    }
   ],
   "source": [
    "# @gin.configurable\n",
    "def to_timestamp(date_str: str) -> pd.Timestamp:\n",
    "  \"\"\"Utilty macro for gin config.\"\"\"\n",
    "  return pd.Timestamp(date_str)\n",
    "\n",
    "\n",
    "# @gin.configurable\n",
    "def local_time(time_str: str) -> pd.Timedelta:\n",
    "  \"\"\"Utilty macro for gin config.\"\"\"\n",
    "  return pd.Timedelta(time_str)\n",
    "\n",
    "\n",
    "# @gin.configurable\n",
    "def enumerate_zones(\n",
    "    n_building_x: int, n_building_y: int\n",
    ") -> Sequence[tuple[int, int]]:\n",
    "  \"\"\"Utilty macro for gin config.\"\"\"\n",
    "  zone_coordinates = []\n",
    "  for x in range(n_building_x):\n",
    "    for y in range(n_building_y):\n",
    "      zone_coordinates.append((x, y))\n",
    "  return zone_coordinates\n",
    "\n",
    "\n",
    "# @gin.configurable\n",
    "def set_observation_normalization_constants(\n",
    "    field_id: str, sample_mean: float, sample_variance: float\n",
    ") -> smart_control_normalization_pb2.ContinuousVariableInfo:\n",
    "  return smart_control_normalization_pb2.ContinuousVariableInfo(\n",
    "      id=field_id, sample_mean=sample_mean, sample_variance=sample_variance\n",
    "  )\n",
    "\n",
    "\n",
    "# @gin.configurable\n",
    "def set_action_normalization_constants(\n",
    "    min_native_value,\n",
    "    max_native_value,\n",
    "    min_normalized_value,\n",
    "    max_normalized_value,\n",
    ") -> bounded_action_normalizer.BoundedActionNormalizer:\n",
    "  return bounded_action_normalizer.BoundedActionNormalizer(\n",
    "      min_native_value,\n",
    "      max_native_value,\n",
    "      min_normalized_value,\n",
    "      max_normalized_value,\n",
    "  )\n",
    "\n",
    "\n",
    "# @gin.configurable\n",
    "def get_zones_from_config(\n",
    "    configuration_path: str,\n",
    ") -> Sequence[smart_control_building_pb2.ZoneInfo]:\n",
    "  \"\"\"Loads up the zones as a gin macro.\"\"\"\n",
    "  with gin.unlock_config():\n",
    "    reader = reader_lib_google.RecordIoReader(input_dir=configuration_path)\n",
    "    zone_infos = reader.read_zone_infos()\n",
    "    return zone_infos\n",
    "\n",
    "\n",
    "# @gin.configurable\n",
    "def get_devices_from_config(\n",
    "    configuration_path: str,\n",
    ") -> Sequence[smart_control_building_pb2.DeviceInfo]:\n",
    "  \"\"\"Loads up HVAC devices as a gin macro.\"\"\"\n",
    "  with gin.unlock_config():\n",
    "    reader = reader_lib_google.RecordIoReader(input_dir=configuration_path)\n",
    "    device_infos = reader.read_device_infos()\n",
    "    return device_infos\n",
    "\n",
    "# @title Load the environments\n",
    "\n",
    "histogram_parameters_tuples = (\n",
    "        ('zone_air_temperature_sensor',(285., 286., 287., 288, 289., 290., 291., 292., 293., 294., 295., 296., 297., 298., 299., 300.,301,302,303)),\n",
    "        ('supply_air_damper_percentage_command',(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)),\n",
    "        ('supply_air_flowrate_setpoint',( 0., 0.05, .1, .2, .3, .4, .5,  .7,  .9)),\n",
    "    )\n",
    "\n",
    "time_zone = 'US/Pacific'\n",
    "collect_scenario_config = os.path.join(data_path, \"sim_config.gin\")\n",
    "print(collect_scenario_config)\n",
    "eval_scenario_config = os.path.join(data_path, \"sim_config.gin\")\n",
    "print(eval_scenario_config)\n",
    "\n",
    "\n",
    "# TODO: replace single environment with BatchedEnvironment\n",
    "#       after differentiable simulator is complete\n",
    "collect_env = load_environment(collect_scenario_config)\n",
    "\n",
    "# For efficency, set metrics_path to None\n",
    "collect_env._metrics_path = None\n",
    "collect_env._occupancy_normalization_constant = 125.0\n",
    "\n",
    "eval_env = load_environment(eval_scenario_config)\n",
    "# eval_env._label += \"_eval\"\n",
    "eval_env._metrics_path = metrics_path\n",
    "eval_env._occupancy_normalization_constant = 125.0\n",
    "\n",
    "initial_collect_env = load_environment(eval_scenario_config)\n",
    "\n",
    "initial_collect_env._metrics_path = metrics_path\n",
    "initial_collect_env._occupancy_normalization_constant = 125.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c55CehnYR8lY"
   },
   "source": [
    "In the sectioni below, we'll define a function that accepts the envirnment and a policy, and runs a fixed number of episodes. The policy can be a rules-based policy or an RL-based policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "id": "bitzHo5_UbXy"
   },
   "outputs": [],
   "source": [
    "# @title Define a method to execute the policy on the environment.\n",
    "def get_trajectory(time_step, current_action: policy_step.PolicyStep):\n",
    "  \"\"\"Get the trajectory for the current action and time step.\"\"\"\n",
    "  observation = time_step.observation\n",
    "  action = current_action.action\n",
    "  policy_info = ()\n",
    "  reward = time_step.reward\n",
    "  discount = time_step.discount\n",
    "\n",
    "  if time_step.is_first():\n",
    "    traj = trajectory.first(observation, action, policy_info, reward, discount)\n",
    "\n",
    "  elif time_step.is_last():\n",
    "    traj = trajectory.last(observation, action, policy_info, reward, discount)\n",
    "\n",
    "  else:\n",
    "    traj = trajectory.mid(observation, action, policy_info, reward, discount)\n",
    "  return traj\n",
    "\n",
    "\n",
    "def compute_avg_return(\n",
    "    environment,\n",
    "    policy,\n",
    "    num_episodes=1,\n",
    "    time_zone: str = \"US/Pacific\",\n",
    "    render_interval_steps: int = 24,\n",
    "    trajectory_observers=None,\n",
    "):\n",
    "  \"\"\"Computes the average return of the policy on the environment.\n",
    "\n",
    "  Args:\n",
    "    environment: environment.Environment\n",
    "    policy: policy.Policy                                \n",
    "    num_episodes: total number of eposides to run.\n",
    "    time_zone: time zone of the environment\n",
    "    render_interval_steps: Number of steps to take between rendering.\n",
    "    trajectory_observers: list of trajectory observers for use in rendering.\n",
    "  \"\"\"\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "\n",
    "    episode_return = 0.0\n",
    "    t0 = time.time()\n",
    "    epoch = t0\n",
    "\n",
    "    step_id = 0\n",
    "    execution_times = []\n",
    "\n",
    "    while not time_step.is_last():\n",
    "\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "\n",
    "      if trajectory_observers is not None:\n",
    "        traj = get_trajectory(time_step, action_step)\n",
    "        for observer in trajectory_observers:\n",
    "          observer(traj)\n",
    "\n",
    "      episode_return += time_step.reward\n",
    "      t1 = time.time()\n",
    "      dt = t1 - t0\n",
    "      episode_seconds = t1 - epoch\n",
    "      execution_times.append(dt)\n",
    "      sim_time = environment.current_simulation_timestamp.tz_convert(time_zone)\n",
    "\n",
    "      print(\n",
    "          \"Step %5d Sim Time: %s, Reward: %8.2f, Return: %8.2f, Mean Step Time:\"\n",
    "          \" %8.2f s, Episode Time: %8.2f s\"\n",
    "          % (\n",
    "              step_id,\n",
    "              sim_time.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "              time_step.reward,\n",
    "              episode_return,\n",
    "              np.mean(execution_times),\n",
    "              episode_seconds,\n",
    "          )\n",
    "      )\n",
    "\n",
    "      if (step_id > 0) and (step_id % render_interval_steps == 0):\n",
    "        if environment._metrics_path:\n",
    "          clear_output(wait=True)\n",
    "          reader = get_latest_episode_reader(environment._metrics_path)\n",
    "          plot_timeseries_charts(reader, time_zone)\n",
    "        render_env(environment)\n",
    "\n",
    "      t0 = t1\n",
    "      step_id += 1\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86IIF7FrfJ_2"
   },
   "source": [
    "# Rules-based Control (RBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form",
    "id": "X9JR8qze6Yvb"
   },
   "outputs": [],
   "source": [
    "# @title Utils for RBC\n",
    "\n",
    "# We're concerned with controlling Heatpumps/ACs and Hot Water Systems (HWS).\n",
    "class DeviceType(enum.Enum):\n",
    "  AC = 0\n",
    "  HWS = 1\n",
    "\n",
    "\n",
    "SetpointName = str  # Identify the setpoint\n",
    "# Setpoint value.\n",
    "SetpointValue = Union[float, int, bool]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ScheduleEvent:\n",
    "  start_time: pd.Timedelta\n",
    "  device: DeviceType\n",
    "  setpoint_name: SetpointName\n",
    "  setpoint_value: SetpointValue\n",
    "\n",
    "\n",
    "# A schedule is a list of times and setpoints for a device.\n",
    "Schedule = list[ScheduleEvent]\n",
    "ActionSequence = list[tuple[DeviceType, SetpointName]]\n",
    "\n",
    "\n",
    "def to_rad(sin_theta: float, cos_theta: float) -> float:\n",
    "  \"\"\"Converts a sin and cos theta to radians to extract the time.\"\"\"\n",
    "\n",
    "  if sin_theta >= 0 and cos_theta >= 0:\n",
    "    return np.arccos(cos_theta)\n",
    "  elif sin_theta >= 0 and cos_theta < 0:\n",
    "    return np.pi - np.arcsin(sin_theta)\n",
    "  elif sin_theta < 0 and cos_theta < 0:\n",
    "    return np.pi - np.arcsin(sin_theta)\n",
    "  else:\n",
    "    return 2 * np.pi - np.arccos(cos_theta)\n",
    "\n",
    "  return np.arccos(cos_theta) + rad_offset\n",
    "\n",
    "\n",
    "def to_dow(sin_theta: float, cos_theta: float) -> float:\n",
    "  \"\"\"Converts a sin and cos theta to days to extract day of week.\"\"\"\n",
    "  theta = to_rad(sin_theta, cos_theta)\n",
    "  return np.floor(7 * theta / 2 / np.pi)\n",
    "\n",
    "\n",
    "def to_hod(sin_theta: float, cos_theta: float) -> float:\n",
    "  \"\"\"Converts a sin and cos theta to hours to extract hour of day.\"\"\"\n",
    "  theta = to_rad(sin_theta, cos_theta)\n",
    "  return np.floor(24 * theta / 2 / np.pi)\n",
    "\n",
    "\n",
    "def find_schedule_action(\n",
    "    schedule: Schedule,\n",
    "    device: DeviceType,\n",
    "    setpoint_name: SetpointName,\n",
    "    timestamp: pd.Timedelta,\n",
    ") -> SetpointValue:\n",
    "  \"\"\"Finds the action for a schedule event for a time and schedule.\"\"\"\n",
    "\n",
    "  # Get all the schedule events for the device and the setpoint, and turn it\n",
    "  # into a series.\n",
    "  device_schedule_dict = {}\n",
    "  for schedule_event in schedule:\n",
    "    if (\n",
    "        schedule_event.device == device\n",
    "        and schedule_event.setpoint_name == setpoint_name\n",
    "    ):\n",
    "      device_schedule_dict[schedule_event.start_time] = (\n",
    "          schedule_event.setpoint_value\n",
    "      )\n",
    "  device_schedule = pd.Series(device_schedule_dict)\n",
    "\n",
    "  # Get the indexes of the schedule events that fall before the timestamp.\n",
    "\n",
    "  device_schedule_indexes = device_schedule.index[\n",
    "      device_schedule.index <= timestamp\n",
    "  ]\n",
    "\n",
    "  # If are no events preceedding the time, then choose the last\n",
    "  # (assuming it wraps around).\n",
    "  if device_schedule_indexes.empty:\n",
    "    return device_schedule.loc[device_schedule.index[-1]]\n",
    "  else:\n",
    "    return device_schedule.loc[device_schedule_indexes[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "form",
    "id": "QZON8t8w2KF5"
   },
   "outputs": [],
   "source": [
    "# @title Define a schedule policy\n",
    "\n",
    "class SchedulePolicy(tf_policy.TFPolicy):\n",
    "  \"\"\"TF Policy implementation of the Schedule policy.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      time_step_spec,\n",
    "      action_spec: types.NestedTensorSpec,\n",
    "      action_sequence: ActionSequence,\n",
    "      weekday_schedule_events: Schedule,\n",
    "      weekend_holiday_schedule_events: Schedule,\n",
    "      dow_sin_index: int,\n",
    "      dow_cos_index: int,\n",
    "      hod_sin_index: int,\n",
    "      hod_cos_index: int,\n",
    "      action_normalizers,\n",
    "      local_start_time: str = pd.Timestamp,\n",
    "      policy_state_spec: types.NestedTensorSpec = (),\n",
    "      info_spec: types.NestedTensorSpec = (),\n",
    "      training: bool = False,\n",
    "      name: Optional[str] = None,\n",
    "  ):\n",
    "    self.weekday_schedule_events = weekday_schedule_events\n",
    "    self.weekend_holiday_schedule_events = weekend_holiday_schedule_events\n",
    "    self.dow_sin_index = dow_sin_index\n",
    "    self.dow_cos_index = dow_cos_index\n",
    "    self.hod_sin_index = hod_sin_index\n",
    "    self.hod_cos_index = hod_cos_index\n",
    "    self.action_sequence = action_sequence\n",
    "    self.action_normalizers = action_normalizers\n",
    "    self.local_start_time = local_start_time\n",
    "    self.norm_mean = 0.0\n",
    "    self.norm_std = 1.0\n",
    "\n",
    "    policy_state_spec = ()\n",
    "\n",
    "    super().__init__(\n",
    "        time_step_spec=time_step_spec,\n",
    "        action_spec=action_spec,\n",
    "        policy_state_spec=policy_state_spec,\n",
    "        info_spec=info_spec,\n",
    "        clip=False,\n",
    "        observation_and_action_constraint_splitter=None,\n",
    "        name=name,\n",
    "    )\n",
    "\n",
    "  def _normalize_action_map(\n",
    "      self, action_map: dict[tuple[DeviceType, SetpointName], SetpointValue]\n",
    "  ) -> dict[tuple[DeviceType, SetpointName], SetpointValue]:\n",
    "\n",
    "    normalized_action_map = {}\n",
    "\n",
    "    for k, v in action_map.items():\n",
    "      for normalizer_k, normalizer in self.action_normalizers.items():\n",
    "        if normalizer_k.endswith(k[1]):\n",
    "\n",
    "          normed_v = normalizer.agent_value(v)\n",
    "          normalized_action_map[k] = normed_v\n",
    "\n",
    "    return normalized_action_map\n",
    "\n",
    "  def _get_action(\n",
    "      self, time_step\n",
    "  ) -> dict[tuple[DeviceType, SetpointName], SetpointValue]:\n",
    "\n",
    "    observation = time_step.observation\n",
    "    action_spec = cast(tensor_spec.BoundedTensorSpec, self.action_spec)\n",
    "    dow_sin = (observation[self.dow_sin_index] * self.norm_std) + self.norm_mean\n",
    "    dow_cos = (observation[self.dow_cos_index] * self.norm_std) + self.norm_mean\n",
    "    hod_sin = (observation[self.hod_sin_index] * self.norm_std) + self.norm_mean\n",
    "    hod_cos = (observation[self.hod_cos_index] * self.norm_std) + self.norm_mean\n",
    "\n",
    "    dow = to_dow(dow_sin, dow_cos)\n",
    "    hod = to_hod(hod_sin, hod_cos)\n",
    "\n",
    "    timestamp = (\n",
    "        pd.Timedelta(hod, unit='hour') + self.local_start_time.utcoffset()\n",
    "    )\n",
    "\n",
    "    if dow < 5:  # weekday\n",
    "\n",
    "      action_map = {\n",
    "          (tup[0], tup[1]): find_schedule_action(\n",
    "              self.weekday_schedule_events, tup[0], tup[1], timestamp\n",
    "          )\n",
    "          for tup in self.action_sequence\n",
    "      }\n",
    "\n",
    "      return action_map\n",
    "\n",
    "    else:  # Weekend\n",
    "\n",
    "      action_map = {\n",
    "          (tup[0], tup[1]): find_schedule_action(\n",
    "              self.weekend_holiday_schedule_events, tup[0], tup[1], timestamp\n",
    "          )\n",
    "          for tup in self.action_sequence\n",
    "      }\n",
    "\n",
    "      return action_map\n",
    "\n",
    "  def _action(self, time_step, policy_state, seed):\n",
    "    del seed\n",
    "    action_map = self._get_action(time_step)\n",
    "    normalized_action_map = self._normalize_action_map(action_map)\n",
    "\n",
    "    action = np.array(\n",
    "        [\n",
    "            normalized_action_map[device_setpoint]\n",
    "            for device_setpoint in self.action_sequence\n",
    "        ],\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "\n",
    "    t_action = tf.convert_to_tensor(action)\n",
    "    return policy_step.PolicyStep(t_action, (), ())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkQs64KT6qs-"
   },
   "source": [
    "Next, we parameterize the setpoint schedule.\n",
    "\n",
    "We distinguish between weekend and holiday schedules:\n",
    "\n",
    "* For **weekdays, between 6:00 am and 7:00 pm local time** we maintain occupancy conditions:\n",
    "  * AC/Heatpump supply air heating setpoint is 12 C\n",
    "  * Supply water temperarure is 77 C\n",
    "* For **weekday, before 6:00 am and after 7:00 pm local time** we maintain efficiency conditions (setback):\n",
    "  * AC/Heatpump supply air heating setpoint is 0 C\n",
    "  * Supply water temperarure is 42 C\n",
    "\n",
    "* For **weekends and holdidays**, all day, we maintain efficiency conditions (setback):\n",
    "  * AC/Heatpump supply air heating setpoint is 0 C\n",
    "  * Supply water temperarure is 42 C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "form",
    "id": "SpveeGWUf5AK"
   },
   "outputs": [],
   "source": [
    "# @title Configure the schedule parameters\n",
    "\n",
    "hod_cos_index = collect_env._field_names.index('hod_cos_000')\n",
    "hod_sin_index = collect_env._field_names.index('hod_sin_000')\n",
    "dow_cos_index = collect_env._field_names.index('dow_cos_000')\n",
    "dow_sin_index = collect_env._field_names.index('dow_sin_000')\n",
    "\n",
    "\n",
    "# Note that temperatures are specified in Kelvin:\n",
    "weekday_schedule_events = [\n",
    "    ScheduleEvent(\n",
    "        pd.Timedelta(6, unit='hour'),\n",
    "        DeviceType.AC,\n",
    "        'supply_air_heating_temperature_setpoint',\n",
    "        292.0,\n",
    "    ),\n",
    "    ScheduleEvent(\n",
    "        pd.Timedelta(19, unit='hour'),\n",
    "        DeviceType.AC,\n",
    "        'supply_air_heating_temperature_setpoint',\n",
    "        285.0,\n",
    "    ),\n",
    "    ScheduleEvent(\n",
    "        pd.Timedelta(6, unit='hour'),\n",
    "        DeviceType.HWS,\n",
    "        'supply_water_setpoint',\n",
    "        350.0,\n",
    "    ),\n",
    "    ScheduleEvent(\n",
    "        pd.Timedelta(19, unit='hour'),\n",
    "        DeviceType.HWS,\n",
    "        'supply_water_setpoint',\n",
    "        315.0,\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "weekend_holiday_schedule_events = [\n",
    "    ScheduleEvent(\n",
    "        pd.Timedelta(6, unit='hour'),\n",
    "        DeviceType.AC,\n",
    "        'supply_air_heating_temperature_setpoint',\n",
    "        285.0,\n",
    "    ),\n",
    "    ScheduleEvent(\n",
    "        pd.Timedelta(19, unit='hour'),\n",
    "        DeviceType.AC,\n",
    "        'supply_air_heating_temperature_setpoint',\n",
    "        285.0,\n",
    "    ),\n",
    "    ScheduleEvent(\n",
    "        pd.Timedelta(6, unit='hour'),\n",
    "        DeviceType.HWS,\n",
    "        'supply_water_setpoint',\n",
    "        315.0,\n",
    "    ),\n",
    "    ScheduleEvent(\n",
    "        pd.Timedelta(19, unit='hour'),\n",
    "        DeviceType.HWS,\n",
    "        'supply_water_setpoint',\n",
    "        315.0,\n",
    "    ),\n",
    "]\n",
    "\n",
    "action_sequence = [\n",
    "    (DeviceType.HWS, 'supply_water_setpoint'),\n",
    "    (DeviceType.AC, 'supply_air_heating_temperature_setpoint'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOTP9p8-0N0H"
   },
   "source": [
    "We instantiate the schedule policy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "form",
    "id": "jv-1GBFTieNc"
   },
   "outputs": [],
   "source": [
    "# @title Instantiate the Schedule RBC policy\n",
    "ts = collect_env.reset()\n",
    "local_start_time = collect_env.current_simulation_timestamp.tz_convert(tz = 'US/Pacific')\n",
    "\n",
    "action_normalizers = collect_env._action_normalizers\n",
    "\n",
    "observation_spec, action_spec, time_step_spec = spec_utils.get_tensor_specs(collect_env)\n",
    "schedule_policy = SchedulePolicy(\n",
    "    time_step_spec= time_step_spec,\n",
    "    action_spec= action_spec,\n",
    "    action_sequence = action_sequence,\n",
    "    weekday_schedule_events = weekday_schedule_events,\n",
    "    weekend_holiday_schedule_events = weekend_holiday_schedule_events,\n",
    "    dow_sin_index=dow_sin_index,\n",
    "    dow_cos_index=dow_cos_index,\n",
    "    hod_sin_index=hod_sin_index,\n",
    "    hod_cos_index=hod_cos_index,\n",
    "    local_start_time=local_start_time,\n",
    "    action_normalizers=action_normalizers,\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAYOf5Xtzi2u"
   },
   "source": [
    "Next, we will run the static control setpoints on the environment to establish baseline performance.\n",
    "\n",
    "**Note:** This will take some time to execute. Feel free to skip this step if you want to jump directly to the RL section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "3Zv-lSiWDp50"
   },
   "outputs": [],
   "source": [
    "# @title Optionally, execute the schedule policy on the environment\n",
    "# Optional\n",
    "compute_avg_return(eval_env, schedule_policy, 1, time_zone=\"US/Pacific\", render_interval_steps=144, trajectory_observers=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDgizVLzRti1"
   },
   "source": [
    "# Reinforcement Learning Control\n",
    "In the previous section we used a simple schedule to control the HVAC setpoints, however in this section, we configure and train a Reinforcement Learning (RL) agent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9g6pE6v2bb8O"
   },
   "source": [
    "Set the configuration parameters for the PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "form",
    "id": "CeVkerwYcng2"
   },
   "outputs": [],
   "source": [
    "# @title Set the RL Agent's parameters\n",
    "\n",
    "# Actor network fully connected layers.\n",
    "actor_fc_layers = (128, 128)\n",
    "\n",
    "# Value network observation fully connected layers.\n",
    "value_fc_layers = (128, 64)\n",
    "\n",
    "num_parallel_environments=1  \n",
    "# Training params\n",
    "actor_fc_layers=(128, 128)\n",
    "value_fc_layers=(128, 64)\n",
    "actor_learning_rate=3e-4\n",
    "minibatch_size=25 # TODO\n",
    "num_epochs=10\n",
    "\n",
    "importance_ratio_clipping=0.2\n",
    "lambda_value=0.95\n",
    "discount_factor=0.99\n",
    "entropy_regularization=0.0\n",
    "use_gae=True\n",
    "use_td_lambda_return=True\n",
    "gradient_clipping=0.5\n",
    "value_clipping=None\n",
    "\n",
    "policy_save_interval=5000\n",
    "summary_interval=1\n",
    "eval_interval=10000\n",
    "eval_episodes=100\n",
    "debug_summaries=True\n",
    "summarize_grads_and_vars=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhTPXjtebMZD"
   },
   "source": [
    "## Initialize the PPO agent\n",
    "\n",
    "The algorihtm Proximal Policy Optimization (PPO) is first introduced by Schulman et al in the paper Proximal Policy Optimization Algorithms (https://arxiv.org/abs/1707.06347) in 2017.\n",
    "\n",
    "In this notebook we illustrate the use of the buidling control environment using the PPO implementation in [TF-Agents](https://www.tensorflow.org/agents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "form",
    "id": "NW0pzLvjbSnP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/derek/sbsim/.venv/lib/python3.10/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer Orthogonal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# @title Construct the PPO agent\n",
    "actor_net = ppo_actor_network.PPOActorNetwork().create_sequential_actor_net(\n",
    "    fc_layer_units=actor_fc_layers,\n",
    "    action_tensor_spec=action_spec,\n",
    ")\n",
    "\n",
    "value_net = value_network.ValueNetwork(\n",
    "    input_tensor_spec=observation_spec,\n",
    "    fc_layer_params=value_fc_layers,\n",
    "    activation_fn=tf.keras.activations.relu\n",
    ")\n",
    "\n",
    "train_step = train_utils.create_train_step()\n",
    "agent = ppo_clip_agent.PPOClipAgent(\n",
    "    time_step_spec=time_step_spec,\n",
    "    action_spec=action_spec,\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=actor_learning_rate),\n",
    "    actor_net=actor_net,\n",
    "    value_net=value_net,\n",
    "    importance_ratio_clipping=importance_ratio_clipping,\n",
    "    lambda_value=lambda_value,\n",
    "    discount_factor=discount_factor,\n",
    "    entropy_regularization=entropy_regularization,\n",
    "    # epochs handled in learner\n",
    "    num_epochs=1,\n",
    "    use_gae=use_gae,\n",
    "    use_td_lambda_return=use_td_lambda_return,\n",
    "    gradient_clipping=gradient_clipping,\n",
    "    value_clipping=value_clipping,\n",
    "    # Skips updating normalizers in the agent, as it's handled in the learner.\n",
    "    update_normalizers_in_train=False,\n",
    "    debug_summaries=debug_summaries,\n",
    "    summarize_grads_and_vars=summarize_grads_and_vars,\n",
    "    train_step_counter=train_step,\n",
    "    compute_value_and_advantage_in_train=False # when minibatch_size is used\n",
    ")\n",
    "agent.collect_policy._clip=True # change clip parameter from TFPolicy parent class to clip action tensor\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5hNdgZBG5BZ"
   },
   "source": [
    "Below we construct 2 replay buffers using reverb: one for training one for normalization. The replay buffers are popualted with state-action-reward-state tuples during collect. Thie allows the agent to relive past experiences, and prevents the model from overfitting in the local neighborhood.\n",
    "\n",
    "During traning, the PPO agent learner uses all available sequences collected in the previous round. The learner will break whole epsisode of expereince into minibatches and shuffle the experiences (if RNN networks are not used). This helps decorrelate the training data in a way that randomization of a training set would in supervised learning. Otherwise, in most environments the experience in a window of time is highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reverb_checkpoint_dir=/home/derek/sbsim/smart_control/sb_colab_demo/PPO/reverb_checkpoint\n",
      "reverb_server_port=35193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/platform/tfrecord_checkpointer.cc:162]  Initializing TFRecordCheckpointer in /home/derek/sbsim/smart_control/sb_colab_demo/PPO/reverb_checkpoint.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:565] Loading latest checkpoint from /home/derek/sbsim/smart_control/sb_colab_demo/PPO/reverb_checkpoint\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:286] Loading checkpoint from /home/derek/sbsim/smart_control/sb_colab_demo/PPO/reverb_checkpoint/2024-11-13T21:24:55.908357682-05:00\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:295] Loading and verifying metadata of the checkpointed tables.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:341] Metadata for table 'training_table' was successfully loaded and verified.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:341] Metadata for table 'normalization_table' was successfully loaded and verified.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:404] Successfully loaded and verified metadata for all (2) tables. We'll now proceed to read the data referenced by the items in the table.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:447] Still reading compressed trajectory data. 1 records have been read so far.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:447] Still reading compressed trajectory data. 101 records have been read so far.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:447] Still reading compressed trajectory data. 201 records have been read so far.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:457] Completed reading compressed trajectory data. We'll now start assembling the checkpointed tables.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:534] Table training_table and 1 items have been successfully loaded from checkpoint at path /home/derek/sbsim/smart_control/sb_colab_demo/PPO/reverb_checkpoint/2024-11-13T21:24:55.908357682-05:00.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:534] Table normalization_table and 0 items have been successfully loaded from checkpoint at path /home/derek/sbsim/smart_control/sb_colab_demo/PPO/reverb_checkpoint/2024-11-13T21:24:55.908357682-05:00.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:540] Successfully loaded 2 tables from /home/derek/sbsim/smart_control/sb_colab_demo/PPO/reverb_checkpoint/2024-11-13T21:24:55.908357682-05:00\n",
      "[reverb/cc/platform/default/server.cc:71] Started replay server on port 35193\n"
     ]
    }
   ],
   "source": [
    "# sequence_length = int(eval_env.steps_per_episode)\n",
    "sequence_length = 25\n",
    "replay_capacity = num_parallel_environments * sequence_length\n",
    "\n",
    "reverb_checkpoint_dir = output_data_path + \"PPO/reverb_checkpoint\"\n",
    "reverb_port = None\n",
    "print('reverb_checkpoint_dir=%s' %reverb_checkpoint_dir)\n",
    "reverb_checkpointer = reverb.platform.checkpointers_lib.DefaultCheckpointer(\n",
    "    path=reverb_checkpoint_dir\n",
    ")\n",
    "\n",
    "reverb_server = reverb.Server(\n",
    "    [\n",
    "        reverb.Table(  # Replay buffer storing experience for training.\n",
    "            name='training_table',\n",
    "            sampler=reverb.selectors.Fifo(),\n",
    "            remover=reverb.selectors.Fifo(),\n",
    "            rate_limiter=reverb.rate_limiters.MinSize(1),\n",
    "            max_size=replay_capacity,\n",
    "            max_times_sampled=1,\n",
    "        ),\n",
    "        reverb.Table(  # Replay buffer storing experience for normalization.\n",
    "            name='normalization_table',\n",
    "            sampler=reverb.selectors.Fifo(),\n",
    "            remover=reverb.selectors.Fifo(),\n",
    "            rate_limiter=reverb.rate_limiters.MinSize(1),\n",
    "            max_size=replay_capacity,\n",
    "            max_times_sampled=1,\n",
    "        ),\n",
    "    ],\n",
    "    port=reverb_port,\n",
    "    checkpointer=reverb_checkpointer\n",
    ")\n",
    "\n",
    "logging_info('reverb_server_port=%d' %reverb_server.port)\n",
    "reverb_replay_train = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "    agent.collect_data_spec,\n",
    "    sequence_length=sequence_length,\n",
    "    table_name='training_table',\n",
    "    server_address='localhost:{}'.format(reverb_server.port),\n",
    "    max_cycle_length=1,\n",
    "    rate_limiter_timeout_ms=1000,\n",
    ")\n",
    "reverb_replay_normalization = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "    agent.collect_data_spec,\n",
    "    sequence_length=sequence_length,\n",
    "    table_name='normalization_table',\n",
    "    server_address='localhost:{}'.format(reverb_server.port),\n",
    "    max_cycle_length=1,\n",
    "    rate_limiter_timeout_ms=1000,\n",
    ")\n",
    "\n",
    "rb_observer = reverb_utils.ReverbTrajectorySequenceObserver(\n",
    "    reverb_replay_train.py_client,\n",
    "    ['training_table', 'normalization_table'],\n",
    "    sequence_length=sequence_length,\n",
    "    stride_length=sequence_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SH7LQZ_Pd0vY"
   },
   "source": [
    "For simplicity, we'll grab eval and collact policies and give them short variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "form",
    "id": "BwY7StuMkuV4"
   },
   "outputs": [],
   "source": [
    "# @title Access the eval and collect policies\n",
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6klSPQeGsPLz"
   },
   "source": [
    "In the next section we define observer classes that enable printing model and environment output as the scenario evolves to who you the percentage of the episode, the timestamp in the scenario, cumulative reward, and the execution time.\n",
    "\n",
    "We also provide a plot observer that periodically outputs the performance charts and the temperature gradient across both floors of the buidling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "form",
    "id": "dJ_EMQkZdw8q"
   },
   "outputs": [],
   "source": [
    "# @title Define Observers\n",
    "class RenderAndPlotObserver:\n",
    "  \"\"\"Renders and plots the environment.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      render_interval_steps: int = 10,\n",
    "      environment=None,\n",
    "  ):\n",
    "    self._counter = 0\n",
    "    self._render_interval_steps = render_interval_steps\n",
    "    self._environment = environment\n",
    "    self._cumulative_reward = 0.0\n",
    "\n",
    "    self._start_time = None\n",
    "    if self._environment is not None:\n",
    "      self._num_timesteps_in_episode = (\n",
    "          self._environment._num_timesteps_in_episode\n",
    "      )\n",
    "      self._environment._end_timestamp\n",
    "\n",
    "  def __call__(self, trajectory: trajectory_lib.Trajectory) -> None:\n",
    "\n",
    "    reward = trajectory.reward\n",
    "    self._cumulative_reward += reward\n",
    "    self._counter += 1\n",
    "    if self._start_time is None:\n",
    "      self._start_time = pd.Timestamp.now()\n",
    "\n",
    "    if self._counter % self._render_interval_steps == 0 and self._environment:\n",
    "\n",
    "      execution_time = pd.Timestamp.now() - self._start_time\n",
    "      mean_execution_time = execution_time.total_seconds() / self._counter\n",
    "\n",
    "      clear_output(wait=True)\n",
    "      if self._environment._metrics_path is not None:\n",
    "        reader = get_latest_episode_reader(self._environment._metrics_path)\n",
    "        plot_timeseries_charts(reader, time_zone)\n",
    "\n",
    "      render_env(self._environment)\n",
    "\n",
    "\n",
    "class PrintStatusObserver:\n",
    "  \"\"\"Prints status information.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self, status_interval_steps: int = 1, environment=None, replay_buffer=None\n",
    "  ):\n",
    "    self._counter = 0\n",
    "    self._status_interval_steps = status_interval_steps\n",
    "    self._environment = environment\n",
    "    self._cumulative_reward = 0.0\n",
    "    self._replay_buffer = replay_buffer\n",
    "\n",
    "    self._start_time = None\n",
    "    if self._environment is not None:\n",
    "      self._num_timesteps_in_episode = (\n",
    "          self._environment._num_timesteps_in_episode\n",
    "      )\n",
    "      self._environment._end_timestamp\n",
    "\n",
    "  def __call__(self, trajectory: trajectory_lib.Trajectory) -> None:\n",
    "\n",
    "    reward = trajectory.reward\n",
    "    self._cumulative_reward += reward\n",
    "    self._counter += 1\n",
    "    if self._start_time is None:\n",
    "      self._start_time = pd.Timestamp.now()\n",
    "\n",
    "    if self._counter % self._status_interval_steps == 0 and self._environment:\n",
    "\n",
    "      execution_time = pd.Timestamp.now() - self._start_time\n",
    "      mean_execution_time = execution_time.total_seconds() / self._counter\n",
    "\n",
    "      sim_time = self._environment.current_simulation_timestamp.tz_convert(\n",
    "          time_zone\n",
    "      )\n",
    "      percent_complete = int(\n",
    "          100.0 * (self._counter / self._num_timesteps_in_episode)\n",
    "      )\n",
    "\n",
    "      if self._replay_buffer is not None:\n",
    "        rb_size = self._replay_buffer.num_frames()\n",
    "        rb_string = \" Replay Buffer Size: %d\" % rb_size\n",
    "      else:\n",
    "        rb_string = \"\"\n",
    "\n",
    "      print(\n",
    "          \"Step %5d of %5d (%3d%%) Sim Time: %s Reward: %2.2f Cumulative\"\n",
    "          \" Reward: %8.2f Execution Time: %s Mean Execution Time: %3.2fs %s\"\n",
    "          % (\n",
    "              self._environment._step_count,\n",
    "              self._num_timesteps_in_episode,\n",
    "              percent_complete,\n",
    "              sim_time.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "              reward,\n",
    "              self._cumulative_reward,\n",
    "              execution_time,\n",
    "              mean_execution_time,\n",
    "              rb_string,\n",
    "          )\n",
    "      )\n",
    "\n",
    "\n",
    "initial_collect_render_plot_observer = RenderAndPlotObserver(\n",
    "    render_interval_steps=144, environment=initial_collect_env\n",
    ")\n",
    "initial_collect_print_status_observer = PrintStatusObserver(\n",
    "    status_interval_steps=1,\n",
    "    environment=initial_collect_env,\n",
    "    replay_buffer=reverb_replay_train,\n",
    ")\n",
    "collect_render_plot_observer = RenderAndPlotObserver(\n",
    "    render_interval_steps=144, environment=collect_env\n",
    ")\n",
    "collect_print_status_observer = PrintStatusObserver(\n",
    "    status_interval_steps=1,\n",
    "    environment=collect_env,\n",
    "    replay_buffer=reverb_replay_train,\n",
    ")\n",
    "eval_render_plot_observer = RenderAndPlotObserver(\n",
    "    render_interval_steps=144, environment=eval_env\n",
    ")\n",
    "eval_print_status_observer = PrintStatusObserver(\n",
    "    status_interval_steps=1, environment=eval_env, replay_buffer=reverb_replay_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "el732oZItQjO"
   },
   "source": [
    "In the following cell, we shall run the baseline control on the scenario to populate the replay buffer. We will use the schedule policy we build above to simulate training off-policy from recorded telemetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ZGq3SY0kKwsa"
   },
   "outputs": [],
   "source": [
    "# @title Populate the replay buffer with data from baseline control\n",
    "initial_collect_actor = actor.Actor(\n",
    "    initial_collect_env,\n",
    "    schedule_policy,\n",
    "    train_step,\n",
    "    steps_per_run=sequence_length * num_parallel_environments,\n",
    "    observers=[\n",
    "      rb_observer,\n",
    "      initial_collect_print_status_observer, \n",
    "      initial_collect_render_plot_observer\n",
    "    ]\n",
    ")\n",
    "initial_collect_actor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/reverb_service_impl.cc:168] Stored checkpoint to /home/derek/sbsim/smart_control/sb_colab_demo/PPO/reverb_checkpoint/2024-11-13T21:47:04.082607232-05:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/derek/sbsim/smart_control/sb_colab_demo/PPO/reverb_checkpoint/2024-11-13T21:47:04.082607232-05:00'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checkpoint the training table\n",
    "reverb_replay_train.py_client.checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3ZzWxqIunCz"
   },
   "source": [
    "Next wrap the replay buffer into a TF dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellView": "form",
    "id": "ba7bilizt_qW"
   },
   "outputs": [],
   "source": [
    "# @title Make a TF Dataset\n",
    "def training_dataset_fn():\n",
    "    return reverb_replay_train.as_dataset(\n",
    "        sample_batch_size=num_parallel_environments,\n",
    "        sequence_preprocess_fn=agent.preprocess_sequence,\n",
    "    )\n",
    "\n",
    "def normalization_dataset_fn():\n",
    "    return reverb_replay_normalization.as_dataset(\n",
    "        sample_batch_size=num_parallel_environments,\n",
    "        sequence_preprocess_fn=agent.preprocess_sequence,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YqfMl5FuQpf"
   },
   "source": [
    "Here, we extract the collect and evaluation policies for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellView": "form",
    "id": "TzwSaxYkeTh5"
   },
   "outputs": [],
   "source": [
    "# @title Convert the policies into TF Eager Policies\n",
    "\n",
    "tf_collect_policy = agent.collect_policy\n",
    "agent_collect_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "  tf_collect_policy, use_tf_function=True)\n",
    "\n",
    "tf_policy = agent.policy\n",
    "agent_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "  tf_policy, use_tf_function=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtoqyo8Ypn0Q"
   },
   "source": [
    "We will set the interval of saving the policies and writing critic, actor, and alphs losses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "al5HNoiwvYO-"
   },
   "source": [
    "In the following cell we will define the agent learner, a TF-Agents wrapper around the process that performs gradiant-based updates to the actor and value networks in the agent.\n",
    "\n",
    "You should see a statememt that shows you where the policies will be saved to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellView": "form",
    "id": "Ah4oS9HLwOid"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policies will be saved to saved_model_dir: /home/derek/sbsim/policies\n",
      "WARNING:tensorflow:From /home/derek/sbsim/.venv/lib/python3.10/site-packages/tf_agents/train/ppo_learner.py:256: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.counter(...)` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/derek/sbsim/.venv/lib/python3.10/site-packages/tf_agents/train/ppo_learner.py:256: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.counter(...)` instead.\n"
     ]
    }
   ],
   "source": [
    "# @title Define an Agent Learner\n",
    "\n",
    "saved_model_dir = os.path.join(root_dir, learner.POLICY_SAVED_MODEL_DIR)\n",
    "print('Policies will be saved to saved_model_dir: %s' %saved_model_dir)\n",
    "env_step_metric = py_metrics.EnvironmentSteps()\n",
    "learning_triggers = [\n",
    "      triggers.PolicySavedModelTrigger(\n",
    "          saved_model_dir,\n",
    "          agent,\n",
    "          train_step,\n",
    "          interval=policy_save_interval,\n",
    "          metadata_metrics={triggers.ENV_STEP_METADATA_KEY: env_step_metric},\n",
    "      ),\n",
    "      triggers.StepPerSecondLogTrigger(train_step, interval=10),\n",
    "]\n",
    "\n",
    "agent_learner = ppo_learner.PPOLearner(\n",
    "      root_dir,\n",
    "      train_step,\n",
    "      agent,\n",
    "      experience_dataset_fn=training_dataset_fn,\n",
    "      normalization_dataset_fn=normalization_dataset_fn,\n",
    "      num_samples=1,\n",
    "      num_epochs=num_epochs,\n",
    "      triggers=learning_triggers,\n",
    "      minibatch_size=minibatch_size,\n",
    "      shuffle_buffer_size=sequence_length,\n",
    "      summary_interval=summary_interval\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdKA4Jy4YfJM"
   },
   "source": [
    "Next, we will define a *collect actor* and an *eval actor* that wrap the policy and the environment, and can execute and collect metrics.\n",
    "\n",
    "The principal difference between the collect actor and the eval actor, is that the collect actor will choose actions by drawing off the actor network distribution, choosing actions that have a high probability over actions with lower probability. This stochastic property enables the agent explore bettwer actions and improve the policy.\n",
    "\n",
    "However, the eval actor always chooses the action associated with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellView": "form",
    "id": "LWsI9znlqLvh"
   },
   "outputs": [],
   "source": [
    "# @title Define a TF-Agents Actor for collect and eval\n",
    "tf_collect_policy = agent.collect_policy\n",
    "collect_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "    tf_collect_policy, use_tf_function=True\n",
    ")\n",
    "\n",
    "collect_actor = actor.Actor(\n",
    "    collect_env,\n",
    "    collect_policy,\n",
    "    train_step,\n",
    "    steps_per_run=sequence_length,\n",
    "    metrics=actor.collect_metrics(1)+ [env_step_metric],\n",
    "    reference_metrics=[env_step_metric],\n",
    "    summary_dir=os.path.join(root_dir, learner.TRAIN_DIR),\n",
    "    summary_interval=1,\n",
    "    observers=[\n",
    "        rb_observer,\n",
    "        env_step_metric,\n",
    "        collect_print_status_observer,\n",
    "        collect_render_plot_observer,\n",
    "    ],\n",
    ")\n",
    "\n",
    "tf_greedy_policy = greedy_policy.GreedyPolicy(agent.policy)\n",
    "eval_greedy_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "    tf_greedy_policy, use_tf_function=True\n",
    ")\n",
    "\n",
    "eval_actor = actor.Actor(\n",
    "    eval_env,\n",
    "    eval_greedy_policy,\n",
    "    train_step,\n",
    "    episodes_per_run=1,\n",
    "    metrics=actor.eval_metrics(1),\n",
    "    reference_metrics=[env_step_metric],\n",
    "    summary_dir=os.path.join(root_dir, 'eval'),\n",
    "    summary_interval=1,\n",
    "    observers=[eval_print_status_observer, eval_render_plot_observer],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_DN734lZAwE"
   },
   "source": [
    "Finally we're ready to execute the RL training loop with PPO!\n",
    "\n",
    "You can sepcify the total number of trainng iterations. With fewer steps, the model will train more slowly, but more steps may make the agent less stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "cellView": "form",
    "id": "PAlT1f6SWYxq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training.\n",
      "Training iteration:  0\n",
      "Collecting.\n",
      "Step     1 of  4032 (  2%) Sim Time: 2023-07-06 00:05 Reward: -0.02 Cumulative Reward:    -1.83 Execution Time: 0 days 00:15:20.370519 Mean Execution Time: 10.96s  Replay Buffer Size: 0\n",
      "Step     2 of  4032 (  2%) Sim Time: 2023-07-06 00:10 Reward: -0.02 Cumulative Reward:    -1.85 Execution Time: 0 days 00:15:26.799537 Mean Execution Time: 10.90s  Replay Buffer Size: 0\n",
      "Step     3 of  4032 (  2%) Sim Time: 2023-07-06 00:15 Reward: -0.03 Cumulative Reward:    -1.88 Execution Time: 0 days 00:15:34.089601 Mean Execution Time: 10.86s  Replay Buffer Size: 0\n",
      "Step     4 of  4032 (  2%) Sim Time: 2023-07-06 00:20 Reward: -0.02 Cumulative Reward:    -1.90 Execution Time: 0 days 00:15:41.427092 Mean Execution Time: 10.82s  Replay Buffer Size: 0\n",
      "Step     5 of  4032 (  2%) Sim Time: 2023-07-06 00:25 Reward: -0.02 Cumulative Reward:    -1.92 Execution Time: 0 days 00:15:48.560264 Mean Execution Time: 10.78s  Replay Buffer Size: 0\n",
      "Step     6 of  4032 (  2%) Sim Time: 2023-07-06 00:30 Reward: -0.02 Cumulative Reward:    -1.94 Execution Time: 0 days 00:15:55.491056 Mean Execution Time: 10.74s  Replay Buffer Size: 0\n",
      "Step     7 of  4032 (  2%) Sim Time: 2023-07-06 00:35 Reward: -0.02 Cumulative Reward:    -1.96 Execution Time: 0 days 00:16:03.483021 Mean Execution Time: 10.71s  Replay Buffer Size: 0\n",
      "Step     8 of  4032 (  2%) Sim Time: 2023-07-06 00:40 Reward: -0.02 Cumulative Reward:    -1.98 Execution Time: 0 days 00:16:10.185580 Mean Execution Time: 10.66s  Replay Buffer Size: 0\n",
      "Step     9 of  4032 (  2%) Sim Time: 2023-07-06 00:45 Reward: -0.02 Cumulative Reward:    -2.00 Execution Time: 0 days 00:16:16.928409 Mean Execution Time: 10.62s  Replay Buffer Size: 0\n",
      "Step    10 of  4032 (  2%) Sim Time: 2023-07-06 00:50 Reward: -0.02 Cumulative Reward:    -2.03 Execution Time: 0 days 00:16:24.035753 Mean Execution Time: 10.58s  Replay Buffer Size: 0\n",
      "Step    11 of  4032 (  2%) Sim Time: 2023-07-06 00:55 Reward: -0.02 Cumulative Reward:    -2.05 Execution Time: 0 days 00:16:30.820169 Mean Execution Time: 10.54s  Replay Buffer Size: 0\n",
      "Step    12 of  4032 (  2%) Sim Time: 2023-07-06 01:00 Reward: -0.02 Cumulative Reward:    -2.07 Execution Time: 0 days 00:16:37.957587 Mean Execution Time: 10.50s  Replay Buffer Size: 0\n",
      "Step    13 of  4032 (  2%) Sim Time: 2023-07-06 01:05 Reward: -0.02 Cumulative Reward:    -2.09 Execution Time: 0 days 00:16:44.639387 Mean Execution Time: 10.46s  Replay Buffer Size: 0\n",
      "Step    14 of  4032 (  2%) Sim Time: 2023-07-06 01:10 Reward: -0.02 Cumulative Reward:    -2.11 Execution Time: 0 days 00:16:51.552765 Mean Execution Time: 10.43s  Replay Buffer Size: 0\n",
      "Step    15 of  4032 (  2%) Sim Time: 2023-07-06 01:15 Reward: -0.02 Cumulative Reward:    -2.13 Execution Time: 0 days 00:16:58.454914 Mean Execution Time: 10.39s  Replay Buffer Size: 0\n",
      "Step    16 of  4032 (  2%) Sim Time: 2023-07-06 01:20 Reward: -0.02 Cumulative Reward:    -2.15 Execution Time: 0 days 00:17:05.533171 Mean Execution Time: 10.36s  Replay Buffer Size: 0\n",
      "Step    17 of  4032 (  2%) Sim Time: 2023-07-06 01:25 Reward: -0.02 Cumulative Reward:    -2.17 Execution Time: 0 days 00:17:12.619831 Mean Execution Time: 10.33s  Replay Buffer Size: 0\n",
      "Step    18 of  4032 (  2%) Sim Time: 2023-07-06 01:30 Reward: -0.02 Cumulative Reward:    -2.19 Execution Time: 0 days 00:17:20.249602 Mean Execution Time: 10.30s  Replay Buffer Size: 0\n",
      "Step    19 of  4032 (  2%) Sim Time: 2023-07-06 01:35 Reward: -0.02 Cumulative Reward:    -2.22 Execution Time: 0 days 00:17:27.309596 Mean Execution Time: 10.27s  Replay Buffer Size: 0\n",
      "Step    20 of  4032 (  2%) Sim Time: 2023-07-06 01:40 Reward: -0.02 Cumulative Reward:    -2.24 Execution Time: 0 days 00:17:34.117442 Mean Execution Time: 10.23s  Replay Buffer Size: 0\n",
      "Step    21 of  4032 (  2%) Sim Time: 2023-07-06 01:45 Reward: -0.02 Cumulative Reward:    -2.26 Execution Time: 0 days 00:17:40.993901 Mean Execution Time: 10.20s  Replay Buffer Size: 0\n",
      "Step    22 of  4032 (  2%) Sim Time: 2023-07-06 01:50 Reward: -0.02 Cumulative Reward:    -2.28 Execution Time: 0 days 00:17:48.159578 Mean Execution Time: 10.17s  Replay Buffer Size: 0\n",
      "Step    23 of  4032 (  2%) Sim Time: 2023-07-06 01:55 Reward: -0.02 Cumulative Reward:    -2.30 Execution Time: 0 days 00:17:54.874553 Mean Execution Time: 10.14s  Replay Buffer Size: 0\n",
      "Step    24 of  4032 (  2%) Sim Time: 2023-07-06 02:00 Reward: -0.02 Cumulative Reward:    -2.33 Execution Time: 0 days 00:18:02.136724 Mean Execution Time: 10.11s  Replay Buffer Size: 0\n",
      "Step    25 of  4032 (  2%) Sim Time: 2023-07-06 02:05 Reward: -0.02 Cumulative Reward:    -2.35 Execution Time: 0 days 00:18:08.849870 Mean Execution Time: 10.08s  Replay Buffer Size: 0\n",
      "Executing gradient updates with 0 frames.\n",
      "Policy Gradient Loss:   0.16, Value Estimation Loss:   4.40, Clip Fraction:   0.12 \n",
      "Evaluating.\n",
      "Step     1 of  4032 (  0%) Sim Time: 2023-07-06 00:05 Reward: -0.02 Cumulative Reward:    -0.02 Execution Time: 0 days 00:00:00.000004 Mean Execution Time: 0.00s  Replay Buffer Size: 0\n",
      "Step     2 of  4032 (  0%) Sim Time: 2023-07-06 00:10 Reward: -0.02 Cumulative Reward:    -0.04 Execution Time: 0 days 00:00:07.050698 Mean Execution Time: 3.53s  Replay Buffer Size: 0\n",
      "Step     3 of  4032 (  0%) Sim Time: 2023-07-06 00:15 Reward: -0.02 Cumulative Reward:    -0.07 Execution Time: 0 days 00:00:14.351093 Mean Execution Time: 4.78s  Replay Buffer Size: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m     _ \u001b[38;5;241m=\u001b[39m eval_env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Run the eval actor after the training iteration, and get its performance.\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     \u001b[43meval_actor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_and_log\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m rb_observer\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     42\u001b[0m reverb_server\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m~/sbsim/.venv/lib/python3.10/site-packages/tf_agents/train/actor.py:180\u001b[0m, in \u001b[0;36mActor.run_and_log\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_and_log\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 180\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_metrics()\n",
      "File \u001b[0;32m~/sbsim/.venv/lib/python3.10/site-packages/tf_agents/train/actor.py:167\u001b[0m, in \u001b[0;36mActor.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 167\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time_step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_driver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_time_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_policy_state\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    172\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_summaries\n\u001b[1;32m    173\u001b[0m       \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summary_interval \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    174\u001b[0m       \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_summary \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summary_interval\n\u001b[1;32m    175\u001b[0m   ):\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_metric_summaries()\n",
      "File \u001b[0;32m~/sbsim/.venv/lib/python3.10/site-packages/tf_agents/drivers/py_driver.py:120\u001b[0m, in \u001b[0;36mPyDriver.run\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    117\u001b[0m   policy_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mget_initial_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    119\u001b[0m action_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39maction(time_step, policy_state)\n\u001b[0;32m--> 120\u001b[0m next_time_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_step\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# When using observer (for the purpose of training), only the previous\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# policy_state is useful. Therefore substitube it in the PolicyStep and\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# consume it w/ the observer.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m action_step_with_previous_state \u001b[38;5;241m=\u001b[39m action_step\u001b[38;5;241m.\u001b[39m_replace(state\u001b[38;5;241m=\u001b[39mpolicy_state)\n",
      "File \u001b[0;32m~/sbsim/.venv/lib/python3.10/site-packages/tf_agents/environments/py_environment.py:236\u001b[0m, in \u001b[0;36mPyEnvironment.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_time_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_reset(\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_time_step\n\u001b[1;32m    233\u001b[0m ):\n\u001b[1;32m    234\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m--> 236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_time_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_time_step\n",
      "File \u001b[0;32m~/sbsim/smart_control/environment/environment.py:1297\u001b[0m, in \u001b[0;36mEnvironment._step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1291\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics_writer\u001b[38;5;241m.\u001b[39mwrite_action_response(\n\u001b[1;32m   1292\u001b[0m       action_response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_simulation_timestamp\n\u001b[1;32m   1293\u001b[0m   )\n\u001b[1;32m   1295\u001b[0m last_timestamp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_simulation_timestamp\n\u001b[0;32m-> 1297\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1299\u001b[0m observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_observation()\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;66;03m# We need to signal to the Actor that action was rejected and not to\u001b[39;00m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;66;03m# append this observation/action request to the trajectory.\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;66;03m# Since TimeStep cannot be extended and it is checked for NaNs,\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;66;03m# we apply -inf as a reward to indicate the rejection.\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;66;03m# This requires a specialized Actor extension class to handle the\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;66;03m# rejection.\u001b[39;00m\n",
      "File \u001b[0;32m~/sbsim/smart_control/simulator/simulator_building.py:268\u001b[0m, in \u001b[0;36mSimulatorBuilding.wait_time\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns after a certain amount of time.\"\"\"\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# Update the building state.\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_step_sim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sbsim/smart_control/simulator/simulator_flexible_floor_plan.py:150\u001b[0m, in \u001b[0;36mSimulatorFlexibleGeometries.execute_step_sim\u001b[0;34m(self, video_filename)\u001b[0m\n\u001b[1;32m    145\u001b[0m convection_coefficient \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weather_controller\u001b[38;5;241m.\u001b[39mget_air_convection_coefficient(current_ts)\n\u001b[1;32m    147\u001b[0m )\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Update each control volume.\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinite_differences_timestep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mambient_temperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambient_temperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvection_coefficient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvection_coefficient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# Simulate airflow\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_building\u001b[38;5;241m.\u001b[39mapply_convection()\n",
      "File \u001b[0;32m~/sbsim/smart_control/simulator/simulator.py:349\u001b[0m, in \u001b[0;36mSimulator.finite_differences_timestep\u001b[0;34m(self, ambient_temperature, convection_coefficient)\u001b[0m\n\u001b[1;32m    347\u001b[0m converged_successfully \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration_count \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iteration_limit):\n\u001b[0;32m--> 349\u001b[0m   temp_estimate, max_delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_temperature_estimates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtemp_estimate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m      \u001b[49m\u001b[43mambient_temperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambient_temperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m      \u001b[49m\u001b[43mconvection_coefficient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvection_coefficient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m iteration_count \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iteration_warning:\n\u001b[1;32m    355\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    356\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStep \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, not converged in \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m steps, max_delta = \u001b[39m\u001b[38;5;132;01m%3.3f\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    357\u001b[0m         iteration_count,\n\u001b[1;32m    358\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iteration_warning,\n\u001b[1;32m    359\u001b[0m         max_delta,\n\u001b[1;32m    360\u001b[0m     )\n",
      "File \u001b[0;32m~/sbsim/smart_control/simulator/tf_simulator.py:773\u001b[0m, in \u001b[0;36mTFSimulator.update_temperature_estimates\u001b[0;34m(self, temperature_estimates, ambient_temperature, convection_coefficient)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;66;03m# Get the inputs to the equation as Tensors from the building.\u001b[39;00m\n\u001b[1;32m    758\u001b[0m (\n\u001b[1;32m    759\u001b[0m     t_temp,\n\u001b[1;32m    760\u001b[0m     t_temp_old,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    765\u001b[0m     t_z,\n\u001b[1;32m    766\u001b[0m ) \u001b[38;5;241m=\u001b[39m _get_input_tensors(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_building)\n\u001b[1;32m    768\u001b[0m (\n\u001b[1;32m    769\u001b[0m     t_convection_left_edge,\n\u001b[1;32m    770\u001b[0m     t_convection_right_edge,\n\u001b[1;32m    771\u001b[0m     t_convection_top_edge,\n\u001b[1;32m    772\u001b[0m     t_convection_bottom_edge,\n\u001b[0;32m--> 773\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mget_oriented_convection_coefficient_tensors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvection_coefficient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_building\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_boundary_cv_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;66;03m# Create shifted tensor to be able to evaluate neighbors in the equation.\u001b[39;00m\n\u001b[1;32m    780\u001b[0m t_temp_left, t_temp_right, t_temp_above, t_temp_below \u001b[38;5;241m=\u001b[39m _get_neighbor_temps(\n\u001b[1;32m    781\u001b[0m     t_temp, ambient_temperature\n\u001b[1;32m    782\u001b[0m )\n",
      "File \u001b[0;32m~/sbsim/smart_control/simulator/tf_simulator.py:389\u001b[0m, in \u001b[0;36mget_oriented_convection_coefficient_tensors\u001b[0;34m(convection_coefficient_air, shape, boundary_cv_mapping)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv_type\u001b[38;5;241m.\u001b[39mposition \u001b[38;5;241m==\u001b[39m CVPositionType\u001b[38;5;241m.\u001b[39mBOUNDARY:\n\u001b[1;32m    388\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cv_type\u001b[38;5;241m.\u001b[39mboundary \u001b[38;5;241m==\u001b[39m CVBoundaryType\u001b[38;5;241m.\u001b[39mEDGE:\n\u001b[0;32m--> 389\u001b[0m     \u001b[43m_set_edge_convection_coefficient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cv_type\u001b[38;5;241m.\u001b[39mboundary \u001b[38;5;241m==\u001b[39m CVBoundaryType\u001b[38;5;241m.\u001b[39mCORNER:\n\u001b[1;32m    392\u001b[0m     _set_corner_convection_coefficient(cv_type)\n",
      "File \u001b[0;32m~/sbsim/smart_control/simulator/tf_simulator.py:354\u001b[0m, in \u001b[0;36mget_oriented_convection_coefficient_tensors.<locals>._set_edge_convection_coefficient\u001b[0;34m(cv_type)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_oriented_convection_coefficient_tensors\u001b[39m(\n\u001b[1;32m    333\u001b[0m     convection_coefficient_air: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m    334\u001b[0m     shape: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m    335\u001b[0m     boundary_cv_mapping: BoundaryCVMapping,\n\u001b[1;32m    336\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[tf\u001b[38;5;241m.\u001b[39mTensor, tf\u001b[38;5;241m.\u001b[39mTensor, tf\u001b[38;5;241m.\u001b[39mTensor, tf\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    337\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns oriented convection coefficient tensors.\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m  Forced convection (wind) is the primary means of exchanging heat\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m    tensors for convection coeffs: left, right, top, bottom, all of shape\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_edge_convection_coefficient\u001b[39m(cv_type: CVType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    355\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sets the convection coefficient for one CV.\"\"\"\u001b[39;00m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mmatch\u001b[39;00m cv_type\u001b[38;5;241m.\u001b[39medge:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# @title Execute the training loop\n",
    "\n",
    "num_training_iterations = 10\n",
    "\n",
    "# Collect the performance results with the untrained model.\n",
    "# eval_actor.run_and_log()\n",
    "\n",
    "logging_info('Training.')\n",
    "for iter in range(num_training_iterations):\n",
    "    print('Training iteration: ', iter)\n",
    "    # Let the collect actor run, using its stochastic action selection policy.\n",
    "    logging_info(\"Collecting.\")\n",
    "    collect_actor.run()\n",
    "    logging_info(\n",
    "        'Executing gradient updates with %d frames.'\n",
    "        %int(reverb_replay_train.num_frames())\n",
    "    )\n",
    "    # Now, with the additional collectsteps in the replay buffer,\n",
    "    # allow the agent to make additional policy improvements.\n",
    "    rb_observer.reset(write_cached_steps=False)\n",
    "    loss_info = agent_learner.run()\n",
    "    logging_info(\n",
    "        'Policy Gradient Loss: %6.2f, Value Estimation Loss: %6.2f, Clip Fraction: %6.2f '\n",
    "        % (\n",
    "            loss_info.extra.policy_gradient_loss.numpy(),\n",
    "            loss_info.extra.value_estimation_loss.numpy(),\n",
    "            loss_info.extra.clip_fraction.numpy(),\n",
    "        )\n",
    "    )\n",
    "    # clearing buffer after training according to the PPO algorithm\n",
    "    reverb_replay_train.clear()\n",
    "    reverb_replay_normalization.clear()\n",
    "\n",
    "    logging_info('Evaluating.')\n",
    "\n",
    "    _ = eval_env.reset()\n",
    "    # Run the eval actor after the training iteration, and get its performance.\n",
    "    eval_actor.run_and_log()\n",
    "    \n",
    "    \n",
    "rb_observer.close()\n",
    "reverb_server.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_learner._num_samples * agent_learner._num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_learner._minibatch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_learner.num_frames_for_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self._num_samples * self._num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverb_replay_train.num_frames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverb_replay_normalization.num_frames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['training_table', 'normalization_table']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb_observer._table_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ReverbFixedLengthSequenceObserver' object has no attribute '_get_writer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrb_observer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_writer\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ReverbFixedLengthSequenceObserver' object has no attribute '_get_writer'"
     ]
    }
   ],
   "source": [
    "rb_observer._get_writer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ReverbFixedLengthSequenceObserver' object has no attribute '_writer_has_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrb_observer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_writer_has_data\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ReverbFixedLengthSequenceObserver' object has no attribute '_writer_has_data'"
     ]
    }
   ],
   "source": [
    "rb_observer._writer_has_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_env.reset()\n",
    "reverb_replay_train.clear()\n",
    "reverb_replay_normalization.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "L7w-mjPcH7u6",
    "kTtVb9wbRsKU",
    "86IIF7FrfJ_2",
    "SDgizVLzRti1"
   ],
   "last_runtime": {
    "build_target": "",
    "kind": "local"
   },
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1a2nzc-VcaGRTpsEFj3FgqRZY0Lk1dgrW",
     "timestamp": 1705074752110
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
